{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pupil Gluing Characterisation üñåÔ∏è\n",
    "\n",
    "This notebook:\n",
    "1. Calculates the Zernike coefficients on two separate starphire plates\n",
    "2. Calculates the Zernike coefficients on the glued starphire setup\n",
    "\n",
    "It is helpful to the phase retrieval process that data taken for the non-glued plates and glued setup have the same orientation.\n",
    "\n",
    "Existing system aberrations are loaded and can be calculated using System_Aberrations.ipynb\n",
    "________________________________________________________________________________________________\n",
    "**NOTE (on data orientation)**: When modelling dLux layers (aberrated apertures, transmissive layers, etc) - they are orientated w.r.t wavefront POV in direction of propagation. In our setup, the data collected has two orientation flips:\n",
    "1. BFS-U3-200S6M flips image upside down from propagation direction POV (and stores it like this too)\n",
    "2. The pupil plane is focused by a second OAP which reflects propgagtion direction and in-turn flips left-right (and it stores it like this too)\n",
    "\n",
    "i.e. total effect is that data is flipped about origin.\n",
    "________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dLux as dl\n",
    "import dLux.utils as dlu\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsp\n",
    "from jax import vmap  \n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", False)\n",
    "jax.config.update(\"jax_debug_nans\", False)\n",
    "jax.config.update('jax_disable_jit', False)\n",
    "\n",
    "\n",
    "import zodiax as zdx\n",
    "import optax\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.filters import window\n",
    "import skimage as ski\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import PowerNorm\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'inferno'\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"image.origin\"] = 'upper' # true reading of array\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "plt.rcParams[\"axes.titlesize\"] = 18\n",
    "plt.rcParams[\"figure.titlesize\"] = 18\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "\n",
    "# data_dir = \"/import/morgana2/gpir9156/toliman/\"\n",
    "data_dir = \"/media/morgana2/gpir9156/toliman/\"\n",
    "\n",
    "\n",
    "# Detector response (gamma curve)\n",
    "alpha = np.load(data_dir+\"detector/80us_detector/ALPHA_norm.npy\")\n",
    "beta = np.load(data_dir+\"detector/80us_detector/BETA_norm.npy\")\n",
    "gamma = np.load(data_dir+\"detector/80us_detector/GAMMA_norm.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.lib.xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Physical Parameters ---------------------------------------------------------------------#\n",
    "aperture_npix = 512           # Number of pixels across the aperture\n",
    "aperture_diameter = 122e-3    # (m) slightly smaller for mask cap\n",
    "spider_width = 20e-3          # Spider width (m)\n",
    "spider_angle =270             # Spider angle (degrees), clockwise, 0 is spider pointing vertically up\n",
    "coords = dlu.pixel_coords(npixels=aperture_npix, diameter=aperture_diameter)\n",
    "circle = dlu.circle(coords=coords, radius=aperture_diameter/2) \n",
    "\n",
    "# Observations wavelengths (bandpass of 530-640nm)\n",
    "red_laser_wl =  635e-09  # for laser data\n",
    "green_laser_wl = 520e-09  # for laser data\n",
    "wf_npixels = aperture_npix  # Number of pixels across the wavefront\n",
    "wf_diam = aperture_diameter             # Diameter of initial wavefront to propagate wavefront (m)\n",
    "\n",
    "# Detector parameters (BFS-U3-200S6-BD)\n",
    "BFS_px_sep = 2.4e-6 *1e3        # pixel separation (mm)\n",
    "f_det = 1338 # 1300#1350                    # Focal length from OAP2 to detector (mm) \n",
    "px_ang_sep = 2*np.arctan( (BFS_px_sep/2)/f_det ) # angular sep between pixels (rad)\n",
    "\n",
    "# Simulated Detector\n",
    "psf_npix = 40                 # Number of pixels along one dim of the PSF\n",
    "psf_hlf_sz = int(psf_npix/2)             # half window sz of cropped data\n",
    "oversample = 1                 # Oversampling factor for the PSF\n",
    "psf_pixel_scale = dlu.rad2arcsec(px_ang_sep) # arcsec (to match detector plate scale) 80e-4 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In the setup, we have a fiber source. \n",
    "# # Can model the intensity distribution using a Guassian or can just measure it directly\n",
    "# # Measuring directly also captures any mirror imperfections hindering transmission\n",
    "# intensity_dist = np.load(data_dir+\"intensity/15_07_intensity_img_stack.npy\")[0,:]\n",
    "# data = np.flip(intensity_dist)\n",
    "\n",
    "# manual_lim = [1363,4203,386,3214]\n",
    "# data = data[manual_lim[2]:manual_lim[3], manual_lim[0]:manual_lim[1]]\n",
    "# data = (data - data.min())/(data.max()-data.min())\n",
    "\n",
    "# blurred = ski.filters.gaussian(data, sigma=(120, 120))\n",
    "\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(1,3,1)\n",
    "# plt.imshow(data)\n",
    "# plt.title(\"Data - pre-focus\")\n",
    "# plt.colorbar()\n",
    "# plt.subplot(1,3,2)\n",
    "# plt.imshow(blurred)\n",
    "# plt.title(\"Data blurred\")\n",
    "# plt.colorbar()\n",
    "# plt.subplot(1,3,3)\n",
    "# intensity_dist = resize(blurred, (aperture_npix, aperture_npix))\n",
    "# intensity_dist = (intensity_dist - intensity_dist.min())/(intensity_dist.max()-intensity_dist.min()) # re-map from 0-1\n",
    "# plt.title(\"Blurred re-sized\")\n",
    "# plt.imshow(intensity_dist)\n",
    "# plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in directly from work in System_Aberrations notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_dist = jnp.load(data_dir+\"intensity/dithered_12_24_120sig.npy\")\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Blurred re-sized\")\n",
    "plt.imshow(intensity_dist)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Aberrations \n",
    "\n",
    "We have an imperfect system. Load in the system aberrations (calculated using System_Aberrations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would have to run new sys analysis on flipped data\n",
    "# f_aberr = \"data/spider/retrieval_results/16_09_mean_coeffs_combined.npy\"\n",
    "f_aberr = \"data/spider/retrieval_results/16_09_flipped_mean_coeffs_combined.npy\"\n",
    "\n",
    "syst_noll = jnp.arange(4, 15) # only first 14 Zernike modes (excluding piston and tip/tilt) are used to classify syst aberrations\n",
    "syst_coeffs = jnp.load(f_aberr)\n",
    "syst_basis = dlu.zernike_basis(js=syst_noll, diameter=aperture_diameter, coordinates=coords)\n",
    "print(\"System Coefficients (noll {}): {}\".format(syst_noll,syst_coeffs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatingTransmissiveLayer(dl.layers.TransmissiveLayer):\n",
    "    \"\"\"\n",
    "    Base class to hold transmissive layers imbuing them with a transmission and\n",
    "    normalise parameter.\n",
    "\n",
    "    ??? abstract \"UML\"\n",
    "        ![UML](../../assets/uml/TransmissiveLayer.png)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    transmission: Array\n",
    "        The Array of transmission values to be applied to the input wavefront.\n",
    "    normalise: bool\n",
    "        Whether to normalise the wavefront after passing through the optic.\n",
    "    rotation: Array([float])\n",
    "        Single value for rotation of transmissive layer (radians). \n",
    "        Array of shape (1,) (zodiax artefact requires array to \n",
    "        optimise on single value). Rotation applied CW\n",
    "    \"\"\"\n",
    "\n",
    "    rotation: np.array\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self: dl.layers.optical_layers.OpticalLayer,\n",
    "        transmission: np.array = None,\n",
    "        normalise: bool = False,\n",
    "        rotation: np.array = np.array([0.0]),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        transmission: Array = None\n",
    "            The array of transmission values to be applied to the input wavefront.\n",
    "        normalise : bool = False\n",
    "            Whether to normalise the wavefront after passing through the optic.\n",
    "        rotation: Array([float])\n",
    "            Single value for rotation of transmissive layer (radians). \n",
    "            Array of shape (1,) (zodiax artefact requires array to \n",
    "            optimise on single value). Rotation applied CW\n",
    "        \"\"\"\n",
    "        self.rotation = rotation\n",
    "        super().__init__(transmission=transmission, normalise=normalise,**kwargs)\n",
    "\n",
    "    def apply(self: dl.layers.optical_layers.OpticalLayer, wavefront: dl.wavefronts.Wavefront) -> dl.wavefronts.Wavefront:\n",
    "        \"\"\"\n",
    "        Applies the layer to the wavefront.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        wavefront : Wavefront\n",
    "            The wavefront to operate on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        wavefront : Wavefront\n",
    "            The transformed wavefront.\n",
    "        \"\"\"\n",
    "        wavefront *= dlu.rotate(self.transmission, self.rotation) \n",
    "        if self.normalise:\n",
    "            wavefront = wavefront.normalise()\n",
    "        return wavefront\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizablePointSource(dl.sources.Source):\n",
    "    \"\"\"\n",
    "    Identical functionality to dLux PointSource - with a key difference\n",
    "    that the flux parameter is held as a float array (0 grads when calc\n",
    "    with Zodiax otherwise).\n",
    "\n",
    "    ??? abstract \"UML\"\n",
    "        ![UML](../../assets/uml/PointSource.png)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    position : Array, radians\n",
    "        The (x, y) on-sky position of this object.\n",
    "    flux : Array, photons\n",
    "        The flux of the object.\n",
    "    spectrum : Spectrum\n",
    "        The spectrum of this object, represented by a Spectrum object.\n",
    "    \"\"\"\n",
    "\n",
    "    position: jnp.array\n",
    "    flux: jnp.array\n",
    "\n",
    "    def __init__(\n",
    "        self: dl.sources.Source,\n",
    "        wavelengths: jnp.array = None,\n",
    "        position: jnp.array = jnp.zeros(2),\n",
    "        flux: jnp.array = None,\n",
    "        weights: jnp.array = None,\n",
    "        spectrum: dl.Spectrum = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        wavelengths : Array, metres = None\n",
    "            The array of wavelengths at which the spectrum is defined. This input is\n",
    "            ignored if a Spectrum object is provided.\n",
    "        position : Array, radians = np.zeros(2)\n",
    "            The (x, y) on-sky position of this object.\n",
    "        flux : float, photons = 1.\n",
    "            The flux of the object.\n",
    "        spectrum : Spectrum = None\n",
    "            The spectrum of this object, represented by a Spectrum object.\n",
    "        \"\"\"\n",
    "        # Position and Flux\n",
    "        self.position = jnp.asarray(position, dtype=float)\n",
    "        self.flux = jnp.asarray(flux, dtype=float)\n",
    "\n",
    "        if self.position.shape != (2,):\n",
    "            raise ValueError(\"position must be a 1d array of shape (2,).\")\n",
    "\n",
    "        super().__init__(\n",
    "            wavelengths=wavelengths, weights=weights, spectrum=spectrum\n",
    "        )\n",
    "\n",
    "    def model(\n",
    "        self: dl.sources.Source,\n",
    "        optics: dl.optical_systems.BaseOpticalSystem,\n",
    "        return_wf: bool = False,\n",
    "        return_psf: bool = False,\n",
    "    ) -> jnp.array:\n",
    "        \"\"\"\n",
    "        Models the source object through the provided optics.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optics : Optics\n",
    "            The optics through which to model the source object.\n",
    "        return_wf : bool = False\n",
    "            Should the Wavefront object be returned instead of the psf Array?\n",
    "        return_psf : bool = False\n",
    "            Should the PSF object be returned instead of the psf Array?\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        object : Array, Wavefront, PSF\n",
    "            if `return_wf` is False and `return_psf` is False, returns the psf Array.\n",
    "            if `return_wf` is True and `return_psf` is False, returns the Wavefront\n",
    "                object.\n",
    "            if `return_wf` is False and `return_psf` is True, returns the PSF object.\n",
    "        \"\"\"\n",
    "        self = self.normalise()\n",
    "        weights = self.weights * self.flux\n",
    "        return optics.propagate(\n",
    "            self.wavelengths, self.position, weights, return_wf, return_psf\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plate + Spider setup simulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zernike aberrations\n",
    "zernike_indicies = jnp.arange(4, 21) \n",
    "coeffs = jnp.zeros(zernike_indicies.shape)\n",
    "basis = dlu.zernike_basis(js=zernike_indicies, coordinates=coords, diameter=aperture_diameter)\n",
    "\n",
    "optical_systems = []\n",
    "transmissions = []\n",
    "\n",
    "# Using PointSources instead of single PointSource object to overcome float grad issue when solving for flux\n",
    "# green_src = dl.PointSources(wavelengths=[green_laser_wl], flux =jnp.asarray([1e8],dtype=float))\n",
    "# red_src = dl.PointSources(wavelengths=[red_laser_wl], flux =jnp.asarray([1e8],dtype=float))\n",
    "green_src = OptimizablePointSource(wavelengths=[green_laser_wl], flux =jnp.asarray([1e9],dtype=float))\n",
    "red_src = OptimizablePointSource(wavelengths=[red_laser_wl], flux =jnp.asarray([1e9],dtype=float))\n",
    "\n",
    "spider_angles = [270, 180] #0deg is spider pointing vertically up, rotates CW from 0deg\n",
    "optical_systems = []\n",
    "transmissions = []\n",
    "for i in range(len(spider_angles)):\n",
    "    spider = dlu.spider(coords=coords, width=spider_width, angles=[spider_angles[i]])\n",
    "    transmission = dlu.combine([circle, spider])\n",
    "\n",
    "    layers = [\n",
    "        ('SystemAberrations', dl.layers.BasisOptic(basis=syst_basis, coefficients=syst_coeffs, normalise=False)),\n",
    "        ('intensity', dl.layers.TransmissiveLayer(transmission=intensity_dist, normalise=False)),\n",
    "        ('spider', RotatingTransmissiveLayer(transmission=transmission, normalise=False)),\n",
    "        ('aperture', dl.layers.BasisOptic(basis=basis, transmission=None, coefficients=coeffs, normalise=False)),\n",
    "    ]\n",
    "\n",
    "    optics = dl.AngularOpticalSystem(wf_npixels = wf_npixels, \n",
    "                                diameter=wf_diam, \n",
    "                                layers=layers, \n",
    "                                psf_npixels=psf_npix, \n",
    "                                psf_pixel_scale=psf_pixel_scale,\n",
    "                                oversample=oversample)\n",
    "\n",
    "    optical_systems.append(optics)\n",
    "    transmissions.append(transmission)\n",
    "\n",
    "# Construct Optics\n",
    "optics_sp270  = optical_systems[0]\n",
    "optics_sp180 = optical_systems[1]\n",
    "# Check PSF for aberrated system\n",
    "layers = [\n",
    "    ('SystemAberrations', dl.layers.BasisOptic(syst_basis, circle*intensity_dist, syst_coeffs, normalise=False)),\n",
    "]\n",
    "\n",
    "optics = dl.AngularOpticalSystem(wf_npixels = wf_npixels, \n",
    "                            diameter=wf_diam, \n",
    "                            layers=layers, \n",
    "                            psf_npixels=psf_npix, \n",
    "                            psf_pixel_scale=psf_pixel_scale,\n",
    "                            oversample=oversample)\n",
    "\n",
    "psf = optics.propagate_mono(green_laser_wl)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(psf**0.5)\n",
    "opd = optics.SystemAberrations.eval_basis()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(opd)\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(optics_sp270.spider.transmission*optics_sp270.intensity.transmission)\n",
    "plt.title(\"Spider 270\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(optics_sp180.spider.transmission*optics_sp180.intensity.transmission)\n",
    "plt.title(\"Spider 180\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in single Starphire data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Phase Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version of jax.scipy.stats.logppmf where k can be non-discrete (helpful if data is not discrete photon counts)\n",
    "from jax.scipy.special import xlogy, gammaln\n",
    "from jax._src.lax.lax import _const as _lax_const\n",
    "from jax import lax\n",
    "from jax._src.numpy.util import promote_args_inexact\n",
    "\n",
    "def jax_0_4_24_logpmf(k: jnp.array, mu: jnp.array, loc: jnp.array = 0) -> jnp.array:\n",
    "    r\"\"\"Poisson log probability mass function.\n",
    "\n",
    "    JAX implementation of :obj:`scipy.stats.poisson` ``logpmf``.\n",
    "\n",
    "    The Poisson probability mass function is given by\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        f(k) = e^{-\\mu}\\frac{\\mu^k}{k!}\n",
    "\n",
    "    and is defined for :math:`k \\ge 0` and :math:`\\mu \\ge 0`.\n",
    "\n",
    "    Args:\n",
    "    k: arraylike, value at which to evaluate the PMF\n",
    "    mu: arraylike, distribution shape parameter\n",
    "    loc: arraylike, distribution offset parameter\n",
    "\n",
    "    Returns:\n",
    "    array of logpmf values.\n",
    "\n",
    "    See Also:\n",
    "    - :func:`jax.scipy.stats.poisson.cdf`\n",
    "    - :func:`jax.scipy.stats.poisson.pmf`\n",
    "    \"\"\"\n",
    "    k, mu, loc = promote_args_inexact(\"poisson.logpmf\", k, mu, loc)\n",
    "    zero = _lax_const(k, 0)\n",
    "    x = lax.sub(k, loc)\n",
    "    log_probs = xlogy(x, mu) - gammaln(x + 1) - mu\n",
    "    #   return jnp.where(jnp.logical_or(lax.lt(x, zero),\n",
    "    #                                   lax.ne(jnp.round(k), k)), -jnp.inf, log_probs)\n",
    "\n",
    "    return jnp.where(lax.lt(x, zero), -jnp.inf, log_probs) # key diff in exit conditions to above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    'aperture.coefficients',\n",
    "    # 'spider.rotation',\n",
    "    'source.position',\n",
    "    'source.flux', \n",
    "    ]\n",
    "\n",
    "learning_rate = 1e-9\n",
    "# for glued data\n",
    "optimisers = [\n",
    "            # # optax.adam(learning_rate=learning_rate),\n",
    "            # optax.adam(learning_rate=1e-8),\n",
    "            # # optax.sgd(learning_rate=1e-16),\n",
    "\n",
    "            # # optax.adam(learning_rate=1e-5),\n",
    "            # optax.adam(learning_rate=1e-7),\n",
    "            # optax.adam(learning_rate=1e8),\n",
    "            # optax.adam(learning_rate=learning_rate),\n",
    "            optax.adam(learning_rate=1e-9),\n",
    "            # optax.sgd(learning_rate=1e-16),\n",
    "\n",
    "            # optax.adam(learning_rate=1e-2),\n",
    "            optax.adam(learning_rate=1e-8),\n",
    "            optax.adam(learning_rate=1e8),\n",
    "              ]\n",
    "# for individul plates\n",
    "# optimisers = [\n",
    "#             # # optax.adam(learning_rate=learning_rate),\n",
    "#             # optax.adam(learning_rate=1e-8),\n",
    "#             # # optax.sgd(learning_rate=1e-16),\n",
    "\n",
    "#             # # optax.adam(learning_rate=1e-5),\n",
    "#             # optax.adam(learning_rate=1e-7),\n",
    "#             # optax.adam(learning_rate=1e8),\n",
    "#             # optax.adam(learning_rate=learning_rate),\n",
    "#             optax.adam(learning_rate=1e-9),\n",
    "#             # optax.sgd(learning_rate=1e-16),\n",
    "\n",
    "#             optax.adam(learning_rate=1e-3),\n",
    "#             optax.adam(learning_rate=1e-8),\n",
    "#             optax.adam(learning_rate=1e7),\n",
    "#               ]\n",
    "\n",
    "\n",
    "@zdx.filter_jit\n",
    "@zdx.filter_value_and_grad(params)\n",
    "def loss_fn_poisson(model, data):\n",
    "\n",
    "    simu_psf = model.model()\n",
    "\n",
    "    # loss = -jsp.stats.poisson.logpmf(k=simu_psf, mu=data).sum()\n",
    "    # loss = -jsp.stats.poisson.logpmf(k=data, mu=simu_psf).sum()\n",
    "    loss = -jax_0_4_24_logpmf(k=data, mu=simu_psf).sum()\n",
    "    # loss = ((data-simu_psf)**2).sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "@zdx.filter_jit\n",
    "@zdx.filter_value_and_grad(params)\n",
    "def loss_fn_chi2(model, data):\n",
    "\n",
    "    simu_psf = model.model()\n",
    "\n",
    "    loss = ((simu_psf-data)**2/data).sum()\n",
    "\n",
    "    return loss\n",
    "@zdx.filter_jit\n",
    "@zdx.filter_value_and_grad(params)\n",
    "def loss_fn_diff2(model, data):\n",
    "\n",
    "    simu_psf = model.model()\n",
    "\n",
    "    loss = ((data-simu_psf)**2).sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "@zdx.filter_jit\n",
    "@zdx.filter_value_and_grad(params)\n",
    "def loss_fn_gauss(model, data, stdev):\n",
    "\n",
    "    simu_psf = model.model()\n",
    "\n",
    "    # loss = -jsp.stats.norm.logpdf(x=data, loc=simu_psf, scale=stdev).sum()\n",
    "    loss = -jsp.stats.norm.logpdf(x=simu_psf, loc=data, scale=stdev).sum()\n",
    "\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets loop this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starphire 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location on detector \n",
    "row_start, col_start = 1676, 2600 # Start coord of window on det where data was taken\n",
    "row_len, col_len = 86, 76   # window size of recorded data\n",
    "\n",
    "alpha_cropped = jnp.flip(alpha[row_start:row_start+row_len, col_start:col_start+col_len])\n",
    "beta_cropped = jnp.flip(beta[row_start:row_start+row_len, col_start:col_start+col_len])\n",
    "gamma_cropped = jnp.flip(gamma[row_start:row_start+row_len, col_start:col_start+col_len])\n",
    "\n",
    "# Order in increasing spider angle for each colour separately\n",
    "img_fnames = [\n",
    "    \"non_glued/16_09_starphire1_green_0deg_400us_0gain_img_stack_batch_0.npy\",\n",
    "    \"non_glued/16_09_starphire1_green_90deg_280us_0gain_img_stack_batch_0.npy\",\n",
    "    \"non_glued/16_09_starphire1_red_0deg_294us_0gain_img_stack_batch_0.npy\",\n",
    "    \"non_glued/16_09_starphire1_red_90deg_356us_0gain_img_stack_batch_0.npy\",\n",
    "    \n",
    "]\n",
    "labels = [\n",
    "    \"0deg_green\",\n",
    "    \"90deg_green\",\n",
    "    \"0deg_red\",\n",
    "    \"90deg_red\",\n",
    "]\n",
    "bckgnd_fnames = [\n",
    "    \"non_glued/16_09_starphire1_green_0deg_bckgnd_400us_0gain_img_stack_batch_0.npy\",\n",
    "    \"non_glued/16_09_starphire1_green_90deg_bckgnd_280us_0gain_img_stack_batch_0.npy\",\n",
    "    \"non_glued/16_09_starphire1_green_0deg_bckgnd_400us_0gain_img_stack_batch_0.npy\",\n",
    "    \"non_glued/16_09_starphire1_green_90deg_bckgnd_280us_0gain_img_stack_batch_0.npy\",\n",
    "]\n",
    "still_frame_idxs = [\n",
    "    0, 0, 0, 0 # find_still_frame.py\n",
    "]\n",
    "\n",
    "imgs = jnp.asarray([np.flip(np.load(data_dir+img_fname)) for img_fname in img_fnames])\n",
    "bckgnds = jnp.asarray([np.mean(np.flip(np.load(data_dir+bckgnd_fname)),axis=0) for bckgnd_fname in bckgnd_fnames])  # single bck\n",
    "\n",
    "epochs = [\n",
    "    5000, 5000,5000,5000,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@zdx.filter_jit\n",
    "@zdx.filter_value_and_grad(params)\n",
    "def loss_fn_diff2(model, data):\n",
    "\n",
    "    simu_psf = model.model()\n",
    "\n",
    "    loss = ((data-simu_psf)**2).sum()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising on both spider orientations simultaneously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starphire1_coeffs, starphire1_pos = [], []\n",
    "\n",
    "# 2 loops for 2 diff source colours\n",
    "for i in range(1):\n",
    "    data_idx = 2*i\n",
    "    ###---------------------------- Re-init models ----------------------------###\n",
    "    if i == 0:\n",
    "        source = green_src\n",
    "    else:\n",
    "        source = red_src\n",
    "    instrument_sp180 = dl.Telescope(optics_sp180, ('source', source))\n",
    "    instrument_sp270 = dl.Telescope(optics_sp270, ('source', source))\n",
    "\n",
    "    sim_psfs = [instrument_sp180.model(), instrument_sp270.model()]\n",
    "    norm_psfs = [PowerNorm(0.2, vmax=sim_psf.max(), vmin=sim_psf.min()) for sim_psf in sim_psfs]    \n",
    "\n",
    "    ###------------------------------- Get Data -----------------------------###\n",
    "    data_sp180 = imgs[data_idx][still_frame_idxs[data_idx],:,:] #assuming only two spider configs, given in order of increasing angle for each colour\n",
    "    data_sp270 = imgs[data_idx+1][still_frame_idxs[data_idx+1],:,:]\n",
    "    data_list = [data_sp180, data_sp270]\n",
    "    scaled_data_list = []\n",
    "    for j in range(len(data_list)):\n",
    "        data = data_list[j]\n",
    "        data = data - bckgnds[j]\n",
    "\n",
    "        # Reverse-model detector response\n",
    "        data_remapped = 0.0 + ((1.0 - 0.0)/(data.max()-data.min()))*(data - data.min()) #[0,1] otput range\n",
    "        data = jnp.power((data_remapped-alpha_cropped)/beta_cropped, (1/gamma_cropped)) \n",
    "\n",
    "        # Scale intensity\n",
    "        current_range = data.max() - data.min()\n",
    "        new_range = sim_psfs[j].max() - sim_psfs[j].min()\n",
    "        scaled_data = ( (data - data.min()) * new_range )/current_range + sim_psfs[j].min()\n",
    "\n",
    "        psf_center_idx = jnp.unravel_index(jnp.argmax(scaled_data, axis=None), scaled_data.shape)\n",
    "        scaled_data = scaled_data[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "                                    psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "        \n",
    "        scaled_data_list.append(scaled_data)\n",
    "    ###------------------------------- Phase Retrieval -----------------------------###\n",
    "    configs = [instrument_sp180, instrument_sp270]\n",
    "    optim, opt_state = zdx.get_optimiser(instrument_sp180, params, optimisers) # Needs to be iterable param (i.e. accessible via instrument class)\n",
    "\n",
    "    progress_bar = tqdm(range(2000), desc='Loss: ')\n",
    "\n",
    "    # Run optimisation loop \n",
    "    net_losses, Coeffs, Positions_sp180, Positions_sp270, Fluxes, SpiderAngles= [],[],[],[],[],[]\n",
    "    for j in progress_bar:\n",
    "        grads = None\n",
    "        net_loss = 0\n",
    "        mean_coeff_grads, mean_position_grads, mean_flux_grads = None, None, None # to set initially in loop\n",
    "        pos_grads = [] # update positional gradients separately (right now I've centered data based on brightest pixel\n",
    "                        # but this is not a super robust method, so allow for source position difference)\n",
    "        spider_grads = [] # acc for spider err rotation independently between configs\n",
    "        for k in range(len(configs)):\n",
    "            loss, grads = loss_fn_diff2(model = configs[k], data = scaled_data_list[k]) #loss_fn_poisson(model = configs[k], data = scaled_data_list[k])\n",
    "            # loss, grads = loss_fn_poisson(model = configs[k], data = scaled_data_list[k])\n",
    "            net_loss += loss \n",
    "            \n",
    "            if k == 0:\n",
    "                mean_coeff_grads = grads.aperture.coefficients/len(configs)\n",
    "                mean_position_grads = grads.source.position/len(configs)\n",
    "                mean_flux_grads = grads.source.flux/len(configs)\n",
    "            else:\n",
    "                mean_coeff_grads += grads.aperture.coefficients/len(configs)\n",
    "                mean_position_grads += grads.source.position/len(configs)\n",
    "                mean_flux_grads += grads.source.flux/len(configs)\n",
    "\n",
    "            pos_grads.append(grads.source.position)\n",
    "            spider_grads.append(grads.spider.rotation)\n",
    "\n",
    "        grads = grads.set('aperture.coefficients', mean_coeff_grads)\n",
    "        grads = grads.set('source.position', mean_position_grads)\n",
    "        grads = grads.set('source.flux', mean_flux_grads)\n",
    "\n",
    "        for k in range(len(configs)):\n",
    "            grads = grads.set('source.position', pos_grads[k])\n",
    "            grads = grads.set('spider.rotation', spider_grads[k])\n",
    "            updates, opt_state = optim.update(grads, opt_state)\n",
    "            configs[k] = zdx.apply_updates(configs[k], updates)\n",
    "\n",
    "        net_losses.append(net_loss)\n",
    "        Fluxes.append(configs[0].source.flux)\n",
    "        Coeffs.append(configs[0].aperture.coefficients)\n",
    "        Positions_sp180.append(configs[0].source.position)\n",
    "        Positions_sp270.append(configs[1].source.position)\n",
    "        SpiderAngles.append([configs[0].spider.rotation, configs[1].spider.rotation])\n",
    "\n",
    "        progress_bar.set_postfix({'Loss (combined)': net_loss})\n",
    "\n",
    "    # ###---------------------------------- Plotting ---------------------------------###\n",
    "    # plt.figure(figsize=(12,18))\n",
    "    # plt.subplot(5,3,1)\n",
    "    # plt.plot(np.asarray(Positions_sp180)[:,0], label=\"Position X - sp180\")\n",
    "    # plt.plot(np.asarray(Positions_sp180)[:,1], label=\"Position Y - sp180\")\n",
    "    # plt.plot(np.asarray(Positions_sp270)[:,0], label=\"Position X - sp270\")\n",
    "    # plt.plot(np.asarray(Positions_sp270)[:,1], label=\"Position Y - sp270\")\n",
    "    # plt.title(\"Position\")\n",
    "    # plt.legend()\n",
    "    # plt.subplot(5,3,2)\n",
    "    # arr_coeffs = np.asarray(Coeffs)\n",
    "    # for l in range(len(Coeffs[0])):\n",
    "    #     label = \"Coeff \" + str(zernike_indicies[l])\n",
    "    #     plt.plot(arr_coeffs[:,l], label=label)\n",
    "    # plt.legend()\n",
    "    # plt.subplot(5,3,3)\n",
    "    # plt.plot(np.asarray(Fluxes))\n",
    "    # plt.title(\"Flux\")\n",
    "    # plt.subplot(5,3,4)\n",
    "    # plt.plot(np.array(net_losses))\n",
    "    # ax = plt.gca()\n",
    "    # ax.set_title(\"Training History\")\n",
    "    # ax.set_xlabel(\"Training Epoch\")\n",
    "    # ax.set_ylabel(\"Poisson Log-Likelihood\")\n",
    "\n",
    "    # it = 5\n",
    "    # for k, config in enumerate(configs):\n",
    "    #     plt.subplot(5,3,it)\n",
    "    #     opd = config.aperture.eval_basis()\n",
    "    #     trans = config.spider.transmission\n",
    "    #     plt.imshow(opd*trans, cmap='viridis')\n",
    "    #     plt.title('Retrieved Aberrations')\n",
    "    #     plt.colorbar()\n",
    "    #     it+=1\n",
    "\n",
    "    # for k, config in enumerate(configs):\n",
    "    #     scaled_data = scaled_data_list[k]\n",
    "    #     instrument = config\n",
    "\n",
    "    #     plt.subplot(5,3,it)\n",
    "    #     norm_psf = PowerNorm(0.2, vmax=scaled_data.max(), vmin=scaled_data.min())\n",
    "    #     plt.imshow(scaled_data, norm=norm_psf)\n",
    "    #     plt.colorbar()\n",
    "    #     plt.title('Data - '+labels[data_idx+k])\n",
    "    #     it+=1\n",
    "\n",
    "    #     plt.subplot(5,3,it)\n",
    "    #     model_psf = instrument.model()\n",
    "    #     current_range = model_psf.max() - model_psf.min()\n",
    "    #     new_range = scaled_data.max() - scaled_data.min()\n",
    "    #     model_psf = ( (model_psf - model_psf.min()) * new_range )/current_range + scaled_data.min()\n",
    "    #     norm_psf = PowerNorm(0.2, vmax=model_psf.max(), vmin=model_psf.min())\n",
    "    #     mask = np.ones(scaled_data.shape)\n",
    "    #     mask[scaled_data < 0.01] = 0\n",
    "    #     plt.imshow(model_psf*mask, norm=norm_psf)\n",
    "    #     plt.title('Model')\n",
    "    #     plt.colorbar()\n",
    "    #     it+=1\n",
    "\n",
    "    #     plt.subplot(5,3,it)\n",
    "    #     resid = scaled_data - model_psf\n",
    "    #     plt.imshow(resid, cmap='bwr', vmax = np.abs(resid).max(), vmin = -np.abs(resid).max())\n",
    "    #     plt.colorbar()\n",
    "    #     plt.title('Residuals')\n",
    "    #     it+=1\n",
    "   \n",
    "    # plt.subplot(5,3,it)\n",
    "    # spider_angles_arr = np.asarray(SpiderAngles)\n",
    "    # plt.plot(spider_angles_arr[:,0], label=\"config0\")\n",
    "    # plt.plot(spider_angles_arr[:,1], label=\"config0\")\n",
    "    # plt.title(\"Spider Angle\")\n",
    "    # plt.ylabel(\"Rotation (rad)\")\n",
    "\n",
    "    # starphire1_coeffs.append(configs[0].aperture.coefficients) # identical coeffs for both configs\n",
    "    # starphire1_pos.append([configs[0].source.position, configs[1].source.position])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising on each config individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starphire1_coeffs, starphire1_pos = [], []\n",
    "\n",
    "# # no error bars, individual orientation fitting TODO sems all frames - not sure how to do simulatenous opt with mult images\n",
    "# for i in range(len(img_fnames)): \n",
    "#     ###---------------------------- Re-init model ----------------------------###\n",
    "#     if '_0deg_' in img_fnames[i]:\n",
    "#         optics = optics_sp180\n",
    "#     elif '_90deg_' in img_fnames[i]:\n",
    "#         optics = optics_sp270\n",
    "#     else:\n",
    "#         ValueError(\"Unknown File format\")\n",
    "\n",
    "#     if 'green' in img_fnames[i]:\n",
    "#         src = green_src\n",
    "#     elif 'red' in img_fnames[i]:\n",
    "#         src = red_src\n",
    "#     else:\n",
    "#         ValueError(\"Unknown File format\")\n",
    "\n",
    "#     instrument = dl.Telescope(optics, ('source', src))\n",
    "#     sim_psf = instrument.model()\n",
    "#     norm_psf = PowerNorm(0.2, vmax=sim_psf.max(), vmin=sim_psf.min())\n",
    "\n",
    "#     ###------------------------------- Load data -----------------------------###\n",
    "#     data = np.load(img_fnames[i])\n",
    "#     data = data[still_frame_idxs[i],:,:] # using the most still frame (find_still_frame.py)\n",
    "\n",
    "#     bckgnd = np.load(bckgnd_fnames[i])\n",
    "#     bckgnd = np.mean(bckgnd)\n",
    "#     data = data - bckgnd\n",
    "\n",
    "#     data_remapped = 0.0 + ((1.0 - 0.0)/(data.max()-data.min()))*(data - data.min()) #[0,1] otput range\n",
    "#     data_lin = np.power((data_remapped-np.flip(alpha_cropped))/np.flip(beta_cropped), (1/np.flip(gamma_cropped))) # flip about origin\n",
    "\n",
    "#     # Scale intensity\n",
    "#     data = data_lin\n",
    "#     current_range = data.max() - data.min()\n",
    "#     new_range = sim_psf.max() - sim_psf.min()\n",
    "#     scaled_data = ( (data - data.min()) * new_range )/current_range + sim_psf.min()\n",
    "\n",
    "#     psf_center_idx = np.unravel_index(np.argmax(scaled_data, axis=None), scaled_data.shape)\n",
    "#     scaled_data = scaled_data[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "#                                 psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "\n",
    "#     plt.figure(figsize=(10,5))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.imshow(scaled_data, norm=norm_psf)\n",
    "#     plt.colorbar()\n",
    "#     plt.title(\"Data (scaled)\")\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(sim_psf, norm=norm_psf)\n",
    "#     plt.colorbar()\n",
    "#     plt.title(\"Simulated\")\n",
    "\n",
    "#     ###------------------------------- Phase Retrieval -----------------------------###\n",
    "#     optim, opt_state = zdx.get_optimiser(instrument, params, optimisers) # Needs to be iterable param (i.e. accessible via instrument class)\n",
    "\n",
    "#     progress_bar = tqdm(range(epochs[i]), desc='Loss: ')\n",
    "\n",
    "#     # Run optimisation loop \n",
    "#     net_losses, Coeffs, Positions, Fluxes= [],[],[],[]\n",
    "#     for j in progress_bar:\n",
    "#         poiss_loss, poiss_grads = loss_fn_poisson(model = instrument, data = scaled_data)\n",
    "\n",
    "#         updates, opt_state = optim.update(poiss_grads, opt_state)\n",
    "#         instrument = zdx.apply_updates(instrument, updates) \n",
    "\n",
    "#         net_losses.append(poiss_loss)\n",
    "#         Fluxes.append(instrument.source.flux)\n",
    "#         Coeffs.append(instrument.aperture.coefficients)\n",
    "#         Positions.append(instrument.source.position)\n",
    "\n",
    "#         progress_bar.set_postfix({'Loss': poiss_loss})\n",
    "#     ###---------------------------------- Plotting ---------------------------------###\n",
    "#     plt.figure(figsize=(11,3))\n",
    "#     plt.subplot(1,4,1)\n",
    "#     plt.plot(np.asarray(Positions)[:,0,0], label=\"Position X\")\n",
    "#     plt.plot(np.asarray(Positions)[:,0,1], label=\"Position Y\")\n",
    "#     plt.title(\"Position\")\n",
    "#     plt.legend()\n",
    "#     plt.subplot(1,4,2)\n",
    "#     arr_coeffs = np.asarray(Coeffs)\n",
    "#     for k in range(len(Coeffs[0])):\n",
    "#         label = \"Coeff \" + str(zernike_indicies[k])\n",
    "#         plt.plot(arr_coeffs[:,k], label=label)\n",
    "#     plt.legend()\n",
    "#     plt.subplot(1,4,3)\n",
    "#     plt.plot(np.asarray(Fluxes))\n",
    "#     plt.title(\"Flux\")\n",
    "#     plt.subplot(1,4,4)\n",
    "#     plt.plot(np.array(net_losses))\n",
    "#     ax = plt.gca()\n",
    "#     ax.set_title(\"Training History\")\n",
    "#     ax.set_xlabel(\"Training Epoch\")\n",
    "#     ax.set_ylabel(\"Poisson Log-Likelihood\")\n",
    "\n",
    "\n",
    "#     plt.figure(figsize=(12,10))\n",
    "#     norm_psf = PowerNorm(0.2, vmax=scaled_data.max(), vmin=scaled_data.min())\n",
    "#     plt.subplot(2,2,1)\n",
    "#     plt.imshow(scaled_data, norm=norm_psf)\n",
    "#     plt.colorbar()\n",
    "#     plt.title('Data')\n",
    "\n",
    "#     plt.subplot(2,2,2)\n",
    "#     model_psf = instrument.model()\n",
    "#     current_range = model_psf.max() - model_psf.min()\n",
    "#     new_range = scaled_data.max() - scaled_data.min()\n",
    "#     model_psf = ( (model_psf - model_psf.min()) * new_range )/current_range + scaled_data.min()\n",
    "#     norm_psf = PowerNorm(0.2, vmax=model_psf.max(), vmin=model_psf.min())\n",
    "#     mask = np.ones(scaled_data.shape)\n",
    "#     mask[scaled_data < 0.01] = 0\n",
    "#     plt.imshow(model_psf*mask, norm=norm_psf)\n",
    "#     plt.title('Model')\n",
    "#     plt.colorbar()\n",
    "\n",
    "#     plt.subplot(2,2,3)\n",
    "#     resid = scaled_data - model_psf\n",
    "#     plt.imshow(resid, cmap='bwr', vmax = np.abs(resid).max(), vmin = -np.abs(resid).max())\n",
    "#     plt.colorbar()\n",
    "#     plt.title('Residuals')\n",
    "\n",
    "#     plt.subplot(2,2,4)\n",
    "#     opd = instrument.aperture.eval_basis()\n",
    "#     transmission = instrument.aperture.transmission \n",
    "#     plt.imshow(opd*transmission, cmap='viridis')\n",
    "#     plt.title('Retrieved Aberrations')\n",
    "#     plt.colorbar()\n",
    "\n",
    "\n",
    "#     print(\"Optimised Position: {}\\nFlux: {}\\nCoefficients: {}\".format(instrument.source.position, instrument.source.flux, instrument.aperture.coefficients))\n",
    "\n",
    "#     starphire1_coeffs.append(instrument.aperture.coefficients)\n",
    "#     starphire1_pos.append(instrument.source.position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "# WFE calc excluding piston, tip and tilt\n",
    "plt.figure(figsize=(10,5))\n",
    "wf_errs = []\n",
    "labels = ['green', 'red']\n",
    "# OR (depending on optimisation config)\n",
    "# labels = [\n",
    "#     \"0deg_green\",\n",
    "#     \"90deg_green\",\n",
    "#     \"0deg_red\",\n",
    "#     \"90deg_red\",\n",
    "# ]\n",
    "for i,coeffs in enumerate(starphire1_coeffs):\n",
    "\n",
    "    if 'red' in labels[i]:\n",
    "        wl = red_laser_wl\n",
    "        colour = 'r'\n",
    "    elif 'green' in labels[i]:\n",
    "        wl = green_laser_wl\n",
    "        colour = 'g'\n",
    "    else:\n",
    "        ValueError(\"Unknown Wavelength\")\n",
    "\n",
    "    plt.scatter(zernike_indicies, coeffs, label=labels[i], c=colour)\n",
    "    RMS_wf_err = ((coeffs**2).sum())**0.5\n",
    "\n",
    "    print(\"{} RMS WFE: {:.3f}lambda\".format(labels[i], RMS_wf_err/wl))\n",
    "    wf_errs.append(RMS_wf_err/wl) \n",
    "\n",
    "plt.xlabel(\"Zernike Noll Index\")\n",
    "plt.ylabel(\"Coefficient (m)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "print(\"Mean RMS WFE: {:.3f}lambda\".format(np.mean(wf_errs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Zernike coeff uncertainties once sure this is calc correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starphire 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fnames = [\n",
    "    data_dir+\"non_glued/16_09_starphire2_green_0deg_173us_0gain_img_stack_batch_0.npy\",\n",
    "    data_dir+\"non_glued/16_09_starphire2_green_90deg_294us_0gain_img_stack_batch_0.npy\",\n",
    "    data_dir+\"non_glued/16_09_starphire2_red_0deg_173us_0gain_img_stack_batch_0.npy\",\n",
    "    data_dir+\"non_glued/16_09_starphire2_red_90deg_173us_0gain_img_stack_batch_0.npy\",\n",
    "    \n",
    "]\n",
    "labels = [\n",
    "    \"0deg_green\",\n",
    "    \"90deg_green\",\n",
    "    \"0deg_red\",\n",
    "    \"90deg_red\",\n",
    "]\n",
    "bckgnd_fnames = [\n",
    "    data_dir+\"non_glued/16_09_starphire2_green_0deg_bckgnd_173us_0gain_img_stack_batch_0.npy\",\n",
    "    data_dir+\"non_glued/16_09_starphire2_red_90deg_bckgnd_173us_0gain_img_stack_batch_0.npy\",\n",
    "    data_dir+\"non_glued/16_09_starphire2_green_0deg_bckgnd_173us_0gain_img_stack_batch_0.npy\",\n",
    "    data_dir+\"non_glued/16_09_starphire2_red_90deg_bckgnd_173us_0gain_img_stack_batch_0.npy\",\n",
    "]\n",
    "still_frame_idxs = [\n",
    "    151, 163, 84, 113 # find_still_frame.py\n",
    "]\n",
    "imgs = np.asarray([np.flip(np.load(img_fname)) for img_fname in img_fnames])\n",
    "bckgnds = np.asarray([np.mean(np.flip(np.load(bckgnd_fname)),axis=0) for bckgnd_fname in bckgnd_fnames]) \n",
    "\n",
    "epochs = [\n",
    "    5000, 5000,5000,5000,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starphire2_coeffs, starphire2_pos = [], []\n",
    "\n",
    "# 2 loops for 2 diff source colours\n",
    "for i in range(2):\n",
    "    ###---------------------------- Re-init models ----------------------------###\n",
    "    if i == 0:\n",
    "        source = green_src\n",
    "    else:\n",
    "        source = red_src\n",
    "    instrument_sp180 = dl.Telescope(optics_sp180, ('source', source))\n",
    "    instrument_sp270 = dl.Telescope(optics_sp270, ('source', source))\n",
    "\n",
    "    sim_psfs = [instrument_sp180.model(), instrument_sp270.model()]\n",
    "    norm_psfs = [PowerNorm(0.2, vmax=sim_psf.max(), vmin=sim_psf.min()) for sim_psf in sim_psfs]    \n",
    "\n",
    "    ###------------------------------- Get Data -----------------------------###\n",
    "    data_idx = 2*i\n",
    "    data_sp180 = imgs[data_idx][still_frame_idxs[data_idx],:,:] #assuming only two spider configs, given in order of increasing angle for each colour\n",
    "    data_sp270 = imgs[data_idx+1][still_frame_idxs[data_idx+1],:,:]\n",
    "    data_list = [data_sp180, data_sp270]\n",
    "    scaled_data_list = []\n",
    "    for j in range(len(data_list)):\n",
    "        data = data_list[j]\n",
    "        data = data - bckgnds[j]\n",
    "\n",
    "        # Reverse-model detector response\n",
    "        data_remapped = 0.0 + ((1.0 - 0.0)/(data.max()-data.min()))*(data - data.min()) #[0,1] otput range\n",
    "        data = np.power((data_remapped-alpha_cropped)/beta_cropped, (1/gamma_cropped)) \n",
    "\n",
    "        # Scale intensity\n",
    "        current_range = data.max() - data.min()\n",
    "        new_range = sim_psfs[j].max() - sim_psfs[j].min()\n",
    "        scaled_data = ( (data - data.min()) * new_range )/current_range + sim_psfs[j].min()\n",
    "\n",
    "        psf_center_idx = np.unravel_index(np.argmax(scaled_data, axis=None), scaled_data.shape)\n",
    "        scaled_data = scaled_data[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "                                    psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "        \n",
    "        scaled_data_list.append(scaled_data)\n",
    "    ###------------------------------- Phase Retrieval -----------------------------###\n",
    "    configs = [instrument_sp180, instrument_sp270]\n",
    "    optim, opt_state = zdx.get_optimiser(instrument_sp180, params, optimisers) # Needs to be iterable param (i.e. accessible via instrument class)\n",
    "\n",
    "    progress_bar = tqdm(range(3000), desc='Loss: ')\n",
    "\n",
    "    # Run optimisation loop \n",
    "    net_losses, Coeffs, Positions_sp180, Positions_sp270, Fluxes, SpiderAngles= [],[],[],[],[],[]\n",
    "    for j in progress_bar:\n",
    "        grads = None\n",
    "        net_loss = 0\n",
    "        mean_coeff_grads, mean_position_grads, mean_flux_grads = None, None, None # to set initially in loop\n",
    "        pos_grads = [] # update positional gradients separately (right now I've centered data based on brightest pixel\n",
    "                        # but this is not a super robust method, so allow for source position difference)\n",
    "        spider_grads = [] # acc for spider err rotation independently between configs\n",
    "        for k in range(len(configs)):\n",
    "            loss, grads = loss_fn_diff2(model = configs[k], data = scaled_data_list[k])\n",
    "            net_loss += loss \n",
    "            \n",
    "            if k == 0:\n",
    "                mean_coeff_grads = grads.aperture.coefficients/len(configs)\n",
    "                mean_position_grads = grads.source.position/len(configs)\n",
    "                mean_flux_grads = grads.source.flux/len(configs)\n",
    "            else:\n",
    "                mean_coeff_grads += grads.aperture.coefficients/len(configs)\n",
    "                mean_position_grads += grads.source.position/len(configs)\n",
    "                mean_flux_grads += grads.source.flux/len(configs)\n",
    "\n",
    "            pos_grads.append(grads.source.position)\n",
    "            spider_grads.append(grads.spider.rotation)\n",
    "\n",
    "        grads = grads.set('aperture.coefficients', mean_coeff_grads)\n",
    "        grads = grads.set('source.position', mean_position_grads)\n",
    "        grads = grads.set('source.flux', mean_flux_grads)\n",
    "\n",
    "        for k in range(len(configs)):\n",
    "            grads = grads.set('source.position', pos_grads[k])\n",
    "            grads = grads.set('spider.rotation', spider_grads[k])\n",
    "            updates, opt_state = optim.update(grads, opt_state)\n",
    "            configs[k] = zdx.apply_updates(configs[k], updates)\n",
    "\n",
    "        net_losses.append(net_loss)\n",
    "        Fluxes.append(configs[0].source.flux)\n",
    "        Coeffs.append(configs[0].aperture.coefficients)\n",
    "        Positions_sp180.append(configs[0].source.position)\n",
    "        Positions_sp270.append(configs[1].source.position)\n",
    "        SpiderAngles.append([configs[0].spider.rotation, configs[1].spider.rotation])\n",
    "\n",
    "        progress_bar.set_postfix({'Loss (combined)': net_loss})\n",
    "\n",
    "    ###---------------------------------- Plotting ---------------------------------###\n",
    "    plt.figure(figsize=(12,15))\n",
    "    plt.subplot(5,3,1)\n",
    "    plt.plot(np.asarray(Positions_sp180)[:,0], label=\"Position X - sp180\")\n",
    "    plt.plot(np.asarray(Positions_sp180)[:,1], label=\"Position Y - sp180\")\n",
    "    plt.plot(np.asarray(Positions_sp270)[:,0], label=\"Position X - sp270\")\n",
    "    plt.plot(np.asarray(Positions_sp270)[:,1], label=\"Position Y - sp270\")\n",
    "    plt.title(\"Position\")\n",
    "    plt.legend()\n",
    "    plt.subplot(5,3,2)\n",
    "    arr_coeffs = np.asarray(Coeffs)\n",
    "    for l in range(len(Coeffs[0])):\n",
    "        label = \"Coeff \" + str(zernike_indicies[l])\n",
    "        plt.plot(arr_coeffs[:,l], label=label)\n",
    "    plt.legend()\n",
    "    plt.subplot(5,3,3)\n",
    "    plt.plot(np.asarray(Fluxes))\n",
    "    plt.title(\"Flux\")\n",
    "    plt.subplot(5,3,4)\n",
    "    plt.plot(np.array(net_losses))\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Training History\")\n",
    "    ax.set_xlabel(\"Training Epoch\")\n",
    "    ax.set_ylabel(\"Poisson Log-Likelihood\")\n",
    "\n",
    "    it = 5\n",
    "    for k, config in enumerate(configs):\n",
    "        plt.subplot(5,3,it)\n",
    "        opd = config.aperture.eval_basis()\n",
    "        trans = config.spider.transmission\n",
    "        plt.imshow(opd*trans, cmap='viridis')\n",
    "        plt.title('Retrieved Aberrations')\n",
    "        plt.colorbar()\n",
    "        it+=1\n",
    "\n",
    "    for k, config in enumerate(configs):\n",
    "        scaled_data = scaled_data_list[k]\n",
    "        instrument = config\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        norm_psf = PowerNorm(0.2, vmax=scaled_data.max(), vmin=scaled_data.min())\n",
    "        plt.imshow(scaled_data, norm=norm_psf)\n",
    "        plt.colorbar()\n",
    "        plt.title('Data - '+labels[data_idx+k])\n",
    "        it+=1\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        model_psf = instrument.model()\n",
    "        current_range = model_psf.max() - model_psf.min()\n",
    "        new_range = scaled_data.max() - scaled_data.min()\n",
    "        model_psf = ( (model_psf - model_psf.min()) * new_range )/current_range + scaled_data.min()\n",
    "        norm_psf = PowerNorm(0.2, vmax=model_psf.max(), vmin=model_psf.min())\n",
    "        mask = np.ones(scaled_data.shape)\n",
    "        mask[scaled_data < 0.01] = 0\n",
    "        plt.imshow(model_psf*mask, norm=norm_psf)\n",
    "        plt.title('Model')\n",
    "        plt.colorbar()\n",
    "        it+=1\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        resid = scaled_data - model_psf\n",
    "        plt.imshow(resid, cmap='bwr', vmax = np.abs(resid).max(), vmin = -np.abs(resid).max())\n",
    "        plt.colorbar()\n",
    "        plt.title('Residuals')\n",
    "        it+=1\n",
    "   \n",
    "    plt.subplot(5,3,it)\n",
    "    spider_angles_arr = np.asarray(SpiderAngles)\n",
    "    plt.plot(spider_angles_arr[:,0], label=\"config0\")\n",
    "    plt.plot(spider_angles_arr[:,1], label=\"config0\")\n",
    "    plt.title(\"Spider Angle\")\n",
    "    plt.ylabel(\"Rotation (rad)\")\n",
    "\n",
    "   \n",
    "    starphire2_coeffs.append(configs[0].aperture.coefficients) # identical coeffs for both configs\n",
    "    starphire2_pos.append([configs[0].source.position, configs[1].source.position])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "# WFE calc excluding piston, tip and tilt\n",
    "plt.figure(figsize=(10,5))\n",
    "wf_errs = []\n",
    "labels = ['green', 'red']\n",
    "for i,coeffs in enumerate(starphire2_coeffs):\n",
    "    if 'red' in labels[i]:\n",
    "        wl = red_laser_wl\n",
    "        colour = 'r'\n",
    "    elif 'green' in labels[i]:\n",
    "        wl = green_laser_wl\n",
    "        colour = 'g'\n",
    "    else:\n",
    "        ValueError(\"Unknown Wavelength\")\n",
    "\n",
    "    plt.scatter(zernike_indicies, coeffs, label=labels[i], c=colour)\n",
    "    RMS_wf_err = ((coeffs**2).sum())**0.5\n",
    "\n",
    "    print(\"{} RMS WFE: {:.3f}lambda\".format(labels[i], RMS_wf_err/wl))\n",
    "    wf_errs.append(RMS_wf_err/wl) \n",
    "\n",
    "plt.xlabel(\"Zernike Noll Index\")\n",
    "plt.ylabel(\"Coefficient (m)\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "print(\"Mean RMS WFE: {:.3f}lambda\".format(np.mean(wf_errs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "titles = [\"Green Data\", \"Red Data\"]\n",
    "wls = [green_laser_wl, red_laser_wl]   \n",
    "\n",
    "subplt_it = 1\n",
    "for i in range(len(titles)):\n",
    "    plt.subplot(1,2,subplt_it)\n",
    "    plt.title(titles[i])\n",
    "    plt.scatter(zernike_indicies, starphire1_coeffs[i]/wls[i], label=\"Starphire 1\")\n",
    "    plt.scatter(zernike_indicies, starphire2_coeffs[i]/wls[i], label=\"Starphire 2\")\n",
    "\n",
    "    plt.xlabel(\"Zernike Noll Index\")\n",
    "    plt.ylabel(\"OPD (multiple of wl)\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    subplt_it+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glued plates\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New intensity dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_dist = jnp.load(data_dir+\"intensity/dithered_12_24_120sig.npy\")\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Blurred re-sized\")\n",
    "plt.imshow(intensity_dist)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New syst aberrations (glued data taken three months after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_aberr = \"data/spider/retrieval_results/3_01_mean_coeffs_combined.npy\" #\"data/spider/retrieval_results/17_12_mean_coeffs_combined.npy\"\n",
    "# f_aberr = \"data/spider/retrieval_results/17_12_mean_coeffs_combined.npy\"\n",
    "f_aberr = \"data/spider/retrieval_results/17_12_flipped_mean_coeffs_combined.npy\"\n",
    "\n",
    "\n",
    "syst_noll = jnp.arange(4, 15) # only first 14 Zernike modes (excluding piston and tip/tilt) are used to classify syst aberrations\n",
    "syst_coeffs = jnp.load(f_aberr)\n",
    "syst_basis = dlu.zernike_basis(js=syst_noll, diameter=aperture_diameter, coordinates=coords)\n",
    "print(\"System Coefficients (noll {}): {}\".format(syst_noll,syst_coeffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create base models with new syst aberrations\n",
    "psf_npix = 450                 # Number of pixels along one dim of the PSF\n",
    "psf_hlf_sz = int(psf_npix/2)   # half window sz of cropped data\n",
    "zernike_indicies = jnp.arange(4, 21)\n",
    "coeffs = jnp.zeros(zernike_indicies.shape)\n",
    "basis = dlu.zernike_basis(js=zernike_indicies, coordinates=coords, diameter=aperture_diameter)\n",
    "optical_systems = []\n",
    "transmissions = []\n",
    "for i in range(len(spider_angles)):\n",
    "    spider = dlu.spider(coords=coords, width=spider_width, angles=[spider_angles[i]])\n",
    "    transmission = dlu.combine([circle, spider])\n",
    "\n",
    "    layers = [\n",
    "        ('SystemAberrations', dl.layers.BasisOptic(basis=syst_basis, coefficients=syst_coeffs, normalise=False)),\n",
    "        ('intensity', dl.layers.TransmissiveLayer(transmission=intensity_dist, normalise=False)),\n",
    "        ('spider', RotatingTransmissiveLayer(transmission=transmission, normalise=False)),\n",
    "        ('aperture', dl.layers.BasisOptic(basis=basis, transmission=None, coefficients=coeffs, normalise=False)),\n",
    "    ]\n",
    "    optics = dl.AngularOpticalSystem(wf_npixels = wf_npixels, \n",
    "                                diameter=wf_diam, \n",
    "                                layers=layers, \n",
    "                                psf_npixels=psf_npix, \n",
    "                                psf_pixel_scale=psf_pixel_scale,\n",
    "                                oversample=oversample)\n",
    "\n",
    "    optical_systems.append(optics)\n",
    "    transmissions.append(transmission)\n",
    "\n",
    "# Construct Optics\n",
    "optics_sp270  = optical_systems[0]\n",
    "optics_sp180 = optical_systems[1]\n",
    "\n",
    "# Check PSF for aberrated system\n",
    "layers = [\n",
    "    ('SystemAberrations', dl.layers.BasisOptic(syst_basis, circle*intensity_dist, syst_coeffs, normalise=False)),\n",
    "]\n",
    "\n",
    "optics = dl.AngularOpticalSystem(wf_npixels = wf_npixels, \n",
    "                            diameter=wf_diam, \n",
    "                            layers=layers, \n",
    "                            psf_npixels=psf_npix, \n",
    "                            psf_pixel_scale=psf_pixel_scale,\n",
    "                            oversample=oversample)\n",
    "\n",
    "psf = optics.propagate_mono(green_laser_wl)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(psf**0.5)\n",
    "plt.xlim([200,250])\n",
    "plt.ylim([200,250])\n",
    "opd = optics.SystemAberrations.eval_basis()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(opd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_start, col_start = 1324, 2184 # Start coord of window on det where data was taken\n",
    "row_len, col_len = 510, 568   # window size of recorded data\n",
    "\n",
    "alpha_cropped = jnp.flip(alpha[row_start:row_start+row_len, col_start:col_start+col_len])\n",
    "beta_cropped = jnp.flip(beta[row_start:row_start+row_len, col_start:col_start+col_len])\n",
    "gamma_cropped = jnp.flip(gamma[row_start:row_start+row_len, col_start:col_start+col_len])\n",
    "\n",
    "# Order in increasing spider angle for each colour separately\n",
    "img_fnames = [\n",
    "    \"glued/17_12_green_0deg_img_80us_0gain_img_stack_batch_0.npy\",\n",
    "    \"glued/17_12_green_90deg_img_80us_0gain_img_stack_batch_0.npy\",\n",
    "    \"glued/17_12_red_0deg_img_80us_0gain_img_stack_batch_0.npy\",\n",
    "    \"glued/17_12_red_90deg_img_80us_0gain_img_stack_batch_0.npy\",\n",
    "\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"0deg_green\",\n",
    "    \"90deg_green\",\n",
    "    \"0deg_red\",\n",
    "    \"90deg_red\",\n",
    "]\n",
    "bckgnd_fnames = [\n",
    "    \"glued/17_12_green_0deg_img_bckgnd_80us_0gain_img_stack_batch_0.npy\",\n",
    "    \"glued/17_12_green_90deg_img_bckgnd_80us_0gain_img_stack_batch_0.npy\",\n",
    "    \"glued/17_12_red_0deg_img_bckgnd_80us_0gain_img_stack_batch_0.npy\",\n",
    "    \"glued/17_12_red_90deg_img_bckgnd_80us_0gain_img_stack_batch_0.npy\",\n",
    "]\n",
    "still_frame_idxs = [\n",
    "    0, 0, 0, 0 # find_still_frame.py\n",
    "]\n",
    "\n",
    "imgs = jnp.asarray([np.flip(np.load(data_dir +img_fname)) for img_fname in img_fnames])\n",
    "bckgnds = jnp.asarray([np.mean(np.flip(np.load(data_dir +bckgnd_fname)),axis=0) for bckgnd_fname in bckgnd_fnames])  # single bck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glued_coeffs, glued_source_pos = [], []\n",
    "final_pos, final_spider_angle = [], [] #[0deg green, 90deg green, 0deg red, 90deg red]\n",
    "\n",
    "np.random.seed(0)\n",
    "rand_coeffs = np.random.rand(7)*1e-6\n",
    "coeffs = np.zeros(zernike_indicies.shape)\n",
    "appen = np.zeros(shape=(coeffs.shape[0] - rand_coeffs.shape[0],))\n",
    "rand_coeffs = np.concatenate((rand_coeffs,appen))\n",
    "\n",
    "MCMC_coeffs = np.array([-4.4e-6,2.7e-6,5e-8,6.2e-7,\n",
    "                        -2e-7,\n",
    "                        2.9e-6,\n",
    "                        -1e-6,\n",
    "                        1.1e-7,2e-7,1e-6,-1.15e-6])\n",
    "# MCMC_coeffs = np.array([-3.87e-6,2.12e-6,-9.4e-7,7.3e-7,-2e-7,2.5e-6,-1.4e-6,2.1e-7,5.9e-8,1e-6,-1.15e-6])\n",
    "MCMC_source_pos = np.array([0.2999e-3, 0.117e-3])\n",
    "MCMC_flux = np.array([1.3e10])\n",
    "\n",
    "# 2 loops for 2 diff source colours\n",
    "for i in range(1):\n",
    "    ###--------------------------- Re-init models ----------------------------###\n",
    "    if i == 0:\n",
    "        source = green_src\n",
    "    else:\n",
    "        source = red_src\n",
    "    instrument_sp180 = dl.Telescope(optics_sp180, ('source', source))\n",
    "    instrument_sp270 = dl.Telescope(optics_sp270, ('source', source))\n",
    "\n",
    "    # # start from different aberration point\n",
    "    # instrument_sp180 = instrument_sp180.set(\"aperture.coefficients\", MCMC_coeffs)\n",
    "    # instrument_sp270 = instrument_sp270.set(\"aperture.coefficients\", MCMC_coeffs)\n",
    "    # instrument_sp180 = instrument_sp180.set(\"source.position\", MCMC_source_pos)\n",
    "    # instrument_sp270 = instrument_sp270.set(\"source.position\", MCMC_source_pos)\n",
    "    # instrument_sp180 = instrument_sp180.set(\"source.flux\", MCMC_flux)\n",
    "    # instrument_sp270 = instrument_sp270.set(\"source.flux\", MCMC_flux)\n",
    "\n",
    "    sim_psfs = [instrument_sp180.model(), instrument_sp270.model()]\n",
    "    norm_psfs = [PowerNorm(0.2, vmax=sim_psf.max(), vmin=sim_psf.min()) for sim_psf in sim_psfs]    \n",
    "\n",
    "    ###------------------------------- Get Data -----------------------------###\n",
    "    data_idx = 2*i\n",
    "    data_sp180 = imgs[data_idx]#[still_frame_idxs[data_idx],:,:] #assuming only two spider configs, given in order of increasing angle for each colour\n",
    "    data_sp270 = imgs[data_idx+1]#[still_frame_idxs[data_idx+1],:,:]\n",
    "    data_list = [data_sp180, data_sp270]\n",
    "    scaled_data_list = []\n",
    "    scaled_stdev_list = []\n",
    "    for j in range(len(data_list)):\n",
    "        data = data_list[j]\n",
    "        data = data - bckgnds[j]\n",
    "\n",
    "        # Reverse-model detector response\n",
    "        scaled_cube=[]\n",
    "        for frame in data:\n",
    "            data_remapped = 0.0 + ((1.0 - 0.0)/(frame.max()-frame.min()))*(frame - frame.min()) #[0,1] otput range\n",
    "            data = np.power((data_remapped-alpha_cropped)/beta_cropped, (1/gamma_cropped)) \n",
    "\n",
    "            # Scale intensity\n",
    "            current_range = data.max() - data.min()\n",
    "            new_range = sim_psfs[j].max() - sim_psfs[j].min()\n",
    "            scaled_data = ( (data - data.min()) * new_range )/current_range + sim_psfs[j].min()\n",
    "            scaled_cube.append(scaled_data)\n",
    "            \n",
    "        scaled_cube = np.asarray(scaled_cube)\n",
    "        stdev = np.std(scaled_cube, axis=0)\n",
    "        picked_frame = scaled_cube[0]\n",
    "        psf_center_idx = np.unravel_index(np.argmax(picked_frame, axis=None), picked_frame.shape)\n",
    "        picked_frame = picked_frame[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "                                    psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "        stdev = stdev[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "                                psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "        \n",
    "        \n",
    "        scaled_stdev_list.append(stdev)\n",
    "        scaled_data_list.append(picked_frame)\n",
    "    ###------------------------------- Phase Retrieval -----------------------------###\n",
    "    configs = [instrument_sp180, instrument_sp270]\n",
    "    optim, opt_state = zdx.get_optimiser(instrument_sp180, params, optimisers) # Needs to be iterable param (i.e. accessible via instrument class)\n",
    "\n",
    "    progress_bar = tqdm(range(100000), desc='Loss: ')\n",
    "\n",
    "    # Run optimisation loop \n",
    "    net_losses, Coeffs, Positions_sp180, Positions_sp270, Fluxes, SpiderAngles= [],[],[],[],[],[]\n",
    "    for j in progress_bar:\n",
    "        grads = None\n",
    "        net_loss = 0\n",
    "        mean_coeff_grads, mean_position_grads, mean_flux_grads = None, None, None # to set initially in loop\n",
    "        pos_grads = [] # update positional gradients separately (right now I've centered data based on brightest pixel\n",
    "                        # but this is not a super robust method, so allow for source position difference)\n",
    "        spider_grads = [] # acc for spider err rotation independently between configs\n",
    "        for k in range(len(configs)):\n",
    "            loss, grads =  loss_fn_poisson(model = configs[k], data = scaled_data_list[k])\n",
    "            net_loss += loss \n",
    "            \n",
    "            if k == 0:\n",
    "                mean_coeff_grads = grads.aperture.coefficients/len(configs)\n",
    "                mean_position_grads = grads.source.position/len(configs)\n",
    "                mean_flux_grads = grads.source.flux/len(configs)\n",
    "            else:\n",
    "                mean_coeff_grads += grads.aperture.coefficients/len(configs)\n",
    "                mean_position_grads += grads.source.position/len(configs)\n",
    "                mean_flux_grads += grads.source.flux/len(configs)\n",
    "\n",
    "            pos_grads.append(grads.source.position)\n",
    "            spider_grads.append(grads.spider.rotation)\n",
    "\n",
    "        grads = grads.set('aperture.coefficients', mean_coeff_grads)\n",
    "        # grads = grads.set('source.position', mean_position_grads)\n",
    "        grads = grads.set('source.flux', mean_flux_grads)\n",
    "\n",
    "        for k in range(len(configs)):\n",
    "            grads = grads.set('source.position', pos_grads[k])\n",
    "            grads = grads.set('spider.rotation', spider_grads[k])\n",
    "            updates, opt_state = optim.update(grads, opt_state)\n",
    "            configs[k] = zdx.apply_updates(configs[k], updates)\n",
    "\n",
    "        net_losses.append(net_loss)\n",
    "        Fluxes.append(configs[0].source.flux)\n",
    "        Coeffs.append(configs[0].aperture.coefficients)\n",
    "        Positions_sp180.append(configs[0].source.position)\n",
    "        Positions_sp270.append(configs[1].source.position)\n",
    "        SpiderAngles.append([configs[0].spider.rotation, configs[1].spider.rotation])\n",
    "\n",
    "        progress_bar.set_postfix({'Loss (combined)': net_loss})\n",
    "\n",
    "    ###------------------------------ Correlation Matrix ---------------------------###\n",
    "    corr_matrices = []\n",
    "    for k in range(len(configs)):\n",
    "        cov = zdx.self_covariance_matrix(pytree=configs[k],\n",
    "                                         parameters=['source.flux', 'source.position', 'aperture.coefficients'],\n",
    "                                         loglike_fn=zdx.bayes.poiss_loglike,\n",
    "                                         )\n",
    "        sqrt_cov = cov**0.5 # for stdevs\n",
    "        corr = np.zeros(shape=cov.shape)\n",
    "        for i in range(cov.shape[0]):\n",
    "            for j in range(cov.shape[0]):\n",
    "                corr[i,j] = cov[i,j]/(sqrt_cov[i,i]*sqrt_cov[j,j])\n",
    "\n",
    "        corr_matrices.append(corr)\n",
    "\n",
    "    ###---------------------------------- Plotting ---------------------------------###\n",
    "    plt.figure(figsize=(15,20))\n",
    "    plt.subplot(5,3,1)\n",
    "    plt.plot(np.asarray(Positions_sp180)[:,0], label=\"Position X - sp180\")\n",
    "    plt.plot(np.asarray(Positions_sp180)[:,1], label=\"Position Y - sp180\")\n",
    "    plt.plot(np.asarray(Positions_sp270)[:,0], label=\"Position X - sp270\")\n",
    "    plt.plot(np.asarray(Positions_sp270)[:,1], label=\"Position Y - sp270\")\n",
    "    plt.title(\"Position\")\n",
    "    final_pos.append([np.asarray(Positions_sp180)[-1,0], np.asarray(Positions_sp180)[-1,1]])\n",
    "    final_pos.append([np.asarray(Positions_sp270)[-1,0], np.asarray(Positions_sp270)[-1,1]])\n",
    "\n",
    "    plt.legend()\n",
    "    plt.subplot(5,3,2)\n",
    "    arr_coeffs = np.asarray(Coeffs)\n",
    "    for l in range(len(Coeffs[0])):\n",
    "        label = \"Coeff \" + str(zernike_indicies[l])\n",
    "        plt.plot(arr_coeffs[:,l], label=label)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(5,3,3)\n",
    "    plt.plot(np.asarray(Fluxes))\n",
    "    plt.title(\"Flux\")\n",
    "    plt.subplot(5,3,4)\n",
    "    plt.plot(np.array(net_losses))\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Training History\")\n",
    "    ax.set_xlabel(\"Training Epoch\")\n",
    "    ax.set_ylabel(\"Poisson Log-Likelihood\")\n",
    "\n",
    "    it = 5\n",
    "    for k, config in enumerate(configs):\n",
    "        plt.subplot(5,3,it)\n",
    "        opd = config.aperture.eval_basis()\n",
    "        trans = config.spider.transmission\n",
    "        plt.imshow(opd*trans, cmap='viridis')\n",
    "        plt.title('Retrieved Aberrations')\n",
    "        plt.colorbar()\n",
    "        it+=1\n",
    "\n",
    "    for k, config in enumerate(configs):\n",
    "        scaled_data = scaled_data_list[k]\n",
    "        instrument = config\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        norm_psf = PowerNorm(0.2, vmax=scaled_data.max(), vmin=scaled_data.min())\n",
    "        plt.imshow(scaled_data, norm=norm_psf)\n",
    "        plt.colorbar()\n",
    "        plt.title('Data - '+labels[data_idx+k])\n",
    "        it+=1\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        model_psf = instrument.model()\n",
    "        current_range = model_psf.max() - model_psf.min()\n",
    "        new_range = scaled_data.max() - scaled_data.min()\n",
    "        model_psf = ( (model_psf - model_psf.min()) * new_range )/current_range + scaled_data.min()\n",
    "        norm_psf = PowerNorm(0.2, vmax=model_psf.max(), vmin=model_psf.min())\n",
    "        mask = np.ones(scaled_data.shape)\n",
    "        # mask[scaled_data <= 225] = 0\n",
    "        plt.imshow(model_psf*mask, norm=norm_psf)\n",
    "        plt.title('Model')\n",
    "        plt.colorbar()\n",
    "        it+=1\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        resid = scaled_data - model_psf\n",
    "        plt.imshow(resid, cmap='bwr', vmax = np.abs(resid).max(), vmin = -np.abs(resid).max())\n",
    "        plt.colorbar()\n",
    "        plt.title('Residuals')\n",
    "        it+=1\n",
    "   \n",
    "    plt.subplot(5,3,it)\n",
    "    spider_angles_arr = np.asarray(SpiderAngles)\n",
    "    plt.plot(spider_angles_arr[:,0], label=\"config0\")\n",
    "    plt.plot(spider_angles_arr[:,1], label=\"config1\")\n",
    "    final_spider_angle.append(spider_angles_arr[-1,0])\n",
    "    final_spider_angle.append(spider_angles_arr[-1,1])\n",
    "    plt.title(\"Spider Angle\")\n",
    "    plt.ylabel(\"Rotation (rad)\")\n",
    "    it += 1\n",
    "\n",
    "    for k in range(len(configs)):\n",
    "        plt.subplot(5,3,it)\n",
    "        plt.imshow(corr_matrices[k], cmap='PiYG')#, vmin=-1, vmax=1)'\n",
    "        plt.colorbar(label='Correlation')\n",
    "        plt.title(\"Corr, model \"+str(k))\n",
    "\n",
    "        it+=1\n",
    "\n",
    "   \n",
    "    glued_coeffs.append(configs[0].aperture.coefficients) # identical coeffs for both configs\n",
    "    glued_source_pos.append([configs[0].source.position, configs[1].source.position])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrices = []\n",
    "for k in range(len(configs)):\n",
    "    cov = zdx.self_covariance_matrix(pytree=configs[k],\n",
    "                                        parameters=['source.flux', 'source.position', 'aperture.coefficients'],\n",
    "                                        loglike_fn=zdx.bayes.poiss_loglike,\n",
    "                                        )\n",
    "    sqrt_cov = cov**0.5 # for stdevs\n",
    "    corr = np.zeros(shape=cov.shape)\n",
    "    for i in range(cov.shape[0]):\n",
    "        for j in range(cov.shape[0]):\n",
    "            corr[i,j] = cov[i,j]/(sqrt_cov[i,i]*sqrt_cov[j,j])\n",
    "\n",
    "    corr_matrices.append(corr)\n",
    "\n",
    "###---------------------------------- Plotting ---------------------------------###\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.subplot(5,3,1)\n",
    "plt.plot(np.asarray(Positions_sp180)[:,0], label=\"Position X - sp180\")\n",
    "plt.plot(np.asarray(Positions_sp180)[:,1], label=\"Position Y - sp180\")\n",
    "plt.plot(np.asarray(Positions_sp270)[:,0], label=\"Position X - sp270\")\n",
    "plt.plot(np.asarray(Positions_sp270)[:,1], label=\"Position Y - sp270\")\n",
    "plt.title(\"Position\")\n",
    "final_pos.append([np.asarray(Positions_sp180)[-1,0], np.asarray(Positions_sp180)[-1,1]])\n",
    "final_pos.append([np.asarray(Positions_sp270)[-1,0], np.asarray(Positions_sp270)[-1,1]])\n",
    "\n",
    "plt.legend()\n",
    "plt.subplot(5,3,2)\n",
    "arr_coeffs = np.asarray(Coeffs)\n",
    "for l in range(len(Coeffs[0])):\n",
    "    label = \"Coeff \" + str(zernike_indicies[l])\n",
    "    plt.plot(arr_coeffs[:,l], label=label)\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(5,3,3)\n",
    "plt.plot(np.asarray(Fluxes))\n",
    "plt.title(\"Flux\")\n",
    "plt.subplot(5,3,4)\n",
    "plt.plot(np.array(net_losses))\n",
    "ax = plt.gca()\n",
    "ax.set_title(\"Training History\")\n",
    "ax.set_xlabel(\"Training Epoch\")\n",
    "ax.set_ylabel(\"Poisson Log-Likelihood\")\n",
    "\n",
    "it = 5\n",
    "for k, config in enumerate(configs):\n",
    "    plt.subplot(5,3,it)\n",
    "    opd = config.aperture.eval_basis()\n",
    "    trans = config.spider.transmission\n",
    "    plt.imshow(opd*trans, cmap='viridis')\n",
    "    plt.title('Retrieved Aberrations')\n",
    "    plt.colorbar()\n",
    "    it+=1\n",
    "\n",
    "for k, config in enumerate(configs):\n",
    "    scaled_data = scaled_data_list[k]\n",
    "    instrument = config\n",
    "\n",
    "    plt.subplot(5,3,it)\n",
    "    norm_psf = PowerNorm(0.2, vmax=scaled_data.max(), vmin=scaled_data.min())\n",
    "    plt.imshow(scaled_data, norm=norm_psf)\n",
    "    plt.colorbar()\n",
    "    plt.title('Data - '+labels[data_idx+k])\n",
    "    it+=1\n",
    "\n",
    "    plt.subplot(5,3,it)\n",
    "    model_psf = instrument.model()\n",
    "    current_range = model_psf.max() - model_psf.min()\n",
    "    new_range = scaled_data.max() - scaled_data.min()\n",
    "    model_psf = ( (model_psf - model_psf.min()) * new_range )/current_range + scaled_data.min()\n",
    "    norm_psf = PowerNorm(0.2, vmax=model_psf.max(), vmin=model_psf.min())\n",
    "    mask = np.ones(scaled_data.shape)\n",
    "    mask[scaled_data <= 2] = 0\n",
    "    plt.imshow(model_psf*mask, norm=norm_psf)\n",
    "    plt.title('Model')\n",
    "    plt.colorbar()\n",
    "    it+=1\n",
    "\n",
    "    plt.subplot(5,3,it)\n",
    "    resid = scaled_data - model_psf\n",
    "    plt.imshow(resid, cmap='bwr', vmax = np.abs(resid).max(), vmin = -np.abs(resid).max())\n",
    "    plt.colorbar()\n",
    "    plt.title('Residuals')\n",
    "    it+=1\n",
    "\n",
    "plt.subplot(5,3,it)\n",
    "spider_angles_arr = np.asarray(SpiderAngles)\n",
    "plt.plot(spider_angles_arr[:,0], label=\"config0\")\n",
    "plt.plot(spider_angles_arr[:,1], label=\"config1\")\n",
    "final_spider_angle.append(spider_angles_arr[-1,0])\n",
    "final_spider_angle.append(spider_angles_arr[-1,1])\n",
    "plt.title(\"Spider Angle\")\n",
    "plt.ylabel(\"Rotation (rad)\")\n",
    "it += 1\n",
    "\n",
    "for k in range(len(configs)):\n",
    "    plt.subplot(5,3,it)\n",
    "    plt.imshow(corr_matrices[k], cmap='PiYG')#, vmin=-1, vmax=1)'\n",
    "    plt.colorbar(label='Correlation')\n",
    "    plt.title(\"Corr, model \"+str(k))\n",
    "\n",
    "    it+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coeffs comparison\n",
    "green_glued_coeffs= glued_coeffs[0]\n",
    "red_glued_coeffs = glued_coeffs[1]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Green Data\")\n",
    "plt.scatter(zernike_indicies, green_glued_coeffs/green_laser_wl, label=\"Glued\")\n",
    "plt.xlabel(\"Zernike Noll Index\")\n",
    "plt.ylabel(\"OPD (multiple of wl)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Red Data\")\n",
    "plt.scatter(zernike_indicies, red_glued_coeffs/red_laser_wl, label=\"Glued\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Zernike Noll Index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________\n",
    "### MCMC\n",
    "High dimensional problem - above optimisation is most likely getting stuck in local minimas. Lets send a walker to traverse this hilly terrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro as npy\n",
    "import numpyro.distributions as dist\n",
    "from jax import device_count\n",
    "import chainconsumer as cc\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "print(device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise model + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by modelling a single config, comparing to a single frame of data\n",
    "instrument_sp180 = dl.Telescope(optics_sp180, ('source', green_src))\n",
    "sim_psf = instrument_sp180.model()\n",
    "\n",
    "# sanity check\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(instrument_sp180.intensity.transmission*instrument_sp180.spider.transmission)\n",
    "plt.title(\"Transmission\")\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(instrument_sp180.SystemAberrations.eval_basis())\n",
    "plt.title(\"System Aberrations\")\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(sim_psf**0.2)\n",
    "plt.title(\"Simulated PSF\")\n",
    "plt.colorbar()\n",
    "plt.xlim([200,250])\n",
    "plt.ylim([200,250])\n",
    "\n",
    "data = imgs[0][0,:,:] # arbitrarily picking first frame\n",
    "data = data - bckgnds[0]\n",
    "\n",
    "# Reverse detector non-linearity\n",
    "data_remapped = 0.0 + ((1.0 - 0.0)/(data.max()-data.min()))*(data - data.min()) #[0,1] otput range\n",
    "data = np.power((data_remapped-np.flip(alpha_cropped))/np.flip(beta_cropped), (1/np.flip(gamma_cropped))) # apply inverse gamma fn \n",
    "\n",
    "# Scale intensity\n",
    "current_range = data.max() - data.min()\n",
    "new_range = sim_psf.max() - sim_psf.min()\n",
    "scaled_data = ( (data - data.min()) * new_range )/current_range + sim_psf.min()\n",
    "\n",
    "# center and crop data around max\n",
    "psf_center_idx = np.unravel_index(np.argmax(scaled_data, axis=None), scaled_data.shape)\n",
    "scaled_data = scaled_data[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "                            psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(scaled_data**0.2)\n",
    "plt.title(\"Data PSF\")\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpyro compatibile model() callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpyro MCMC samplers expect jax-framework models\n",
    "n_coeffs = zernike_indicies.shape[0]\n",
    "params = [\n",
    "    'aperture.coefficients',\n",
    "    # 'spider.rotation',\n",
    "    'source.position',\n",
    "    'source.flux', \n",
    "    ]\n",
    "def psf_model(data, model):\n",
    "    values = [\n",
    "        npy.sample(\"coefficients\", dist.Uniform(-3e-5, 3e-5).expand([n_coeffs])),\n",
    "        npy.sample(\"position\", dist.Uniform(-4e-4, 4e-4).expand([2])),\n",
    "        npy.sample(\"flux\", dist.Uniform(1e10,3e10)), # I don't this should be sampled from uniform dist\n",
    "    ]\n",
    "\n",
    "    with npy.plate(\"data\", len(data.flatten())):\n",
    "        poisson_model = dist.Poisson(\n",
    "            model.set(params, values).model().flatten())\n",
    "        return npy.sample(\"psf\", poisson_model, obs=data.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct sampler and send the walker walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = npy.infer.MCMC(\n",
    "    # npy.infer.NUTS(psf_model),    \n",
    "    npy.infer.BarkerMH(psf_model),    \n",
    "    num_warmup=2000,\n",
    "    num_samples=20000,\n",
    "    num_chains=device_count(),\n",
    "    progress_bar=True,\n",
    ")\n",
    "sampler.run(jr.PRNGKey(0), scaled_data, instrument_sp180)\n",
    "\n",
    "# this is really slow :/ (not on glinton <3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if chains have converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.print_summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split vectorised params into single chains - one per parameter in the vector. (useful for plotting corner plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = sampler.get_samples()\n",
    "# import pickle \n",
    "\n",
    "# with open('data/14_1_25_MCMC.pkl', 'wb') as f:\n",
    "#     pickle.dump(samples, f)\n",
    "with open('data/14_1_25_MCMC.pkl', 'rb') as f:\n",
    "    samples = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into separate chains for params grouped in single vector (I think this will change when num_chains!=1)\n",
    "str_var = [\"coefficients\", \"position\"] # name of variable with grouped params\n",
    "prefix = [\"Noll \", \"Position \"] # Corresponding prefix to new split variables \n",
    "new_samples={}\n",
    "for key, value in samples.items():\n",
    "    if key in str_var:\n",
    "        idx = str_var.index(key)\n",
    "        chain_cube = value\n",
    "        for i in range(chain_cube.shape[1]):\n",
    "            if key == \"coefficients\":\n",
    "                new_samples[prefix[idx]+str(zernike_indicies[i])] = chain_cube[:,i]\n",
    "            else:\n",
    "                new_samples[prefix[idx]+str(i)] = chain_cube[:,i]\n",
    "\n",
    "    else:\n",
    "        new_samples[key] = value\n",
    "\n",
    "print(new_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corner plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = cc.Chain(samples=pd.DataFrame.from_dict(new_samples), name=\"Parameter Inference\")\n",
    "consumer = cc.ChainConsumer().add_chain(chain)\n",
    "\n",
    "%matplotlib inline\n",
    "fig = consumer.plotter.plot()\n",
    "fig.set_size_inches((40,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update model with params and see how accurate fit is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCMC_stats = npy.diagnostics.summary(samples=samples, prob=0.9, group_by_chain=False) # 90% CI\n",
    "\n",
    "MCMC_coeffs = MCMC_stats[\"coefficients\"][\"mean\"]\n",
    "MCMC_source_pos = MCMC_stats[\"position\"][\"mean\"]\n",
    "MCMC_flux = jnp.array([MCMC_stats[\"flux\"][\"mean\"]])\n",
    "\n",
    "print(\"Coeffs: {}\\nSource Pos: {}\\nSource Flux: {}\\n\".format(MCMC_coeffs,MCMC_source_pos, MCMC_flux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = instrument_sp180.set(\"aperture.coefficients\", MCMC_coeffs)\n",
    "updated_model =  updated_model.set(\"source.position\", MCMC_source_pos)\n",
    "updated_model = updated_model.set(\"source.flux\", MCMC_flux)\n",
    "\n",
    "print(updated_model.aperture.coefficients)\n",
    "print(updated_model.source.flux)\n",
    "print(updated_model.source.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_psf = updated_model.model()\n",
    "\n",
    "plt.imshow(updated_psf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare to AOS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AOS_data_fname = data_dir+\"zygo_data/20241115_142mmwindowinTB_PHASE_waves.npy\"\n",
    "phase_screen = np.load(AOS_data_fname)\n",
    "\n",
    "center = [int(phase_screen.shape[0]/2)+40, int(phase_screen.shape[1]/2)+10]\n",
    "radius = 520 #(px)\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(phase_screen)\n",
    "plt.title(\"AOS Zygo Phase Data\")\n",
    "plt.colorbar(label=\"Phase (waves)\")\n",
    "plt.subplot(1,3,2)\n",
    "cropped_phase_screen = phase_screen[center[0]-radius:center[0]+radius, center[1]-radius:center[1]+radius]\n",
    "plt.imshow(cropped_phase_screen)\n",
    "plt.title(\"Cropped\")\n",
    "plt.colorbar(label=\"Phase (waves)\")\n",
    "plt.subplot(1,3,3)\n",
    "circ = dlu.circle(coords=coords, radius=aperture_diameter/2)\n",
    "resized_masked_phase_screen = resize(cropped_phase_screen, (aperture_npix, aperture_npix))*circ\n",
    "# resized_masked_phase_screen = dlu.rotate(resized_masked_phase_screen, angle=np.pi*1.0)\n",
    "plt.imshow(resized_masked_phase_screen)\n",
    "plt.title(\"Re-sized + Masked\")\n",
    "plt.colorbar(label=\"Phase (waves)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aos_model = optics_sp180.set(\"psf_npixels\",1000)\n",
    "test_wl = 6.328e-7 # AOS wl (m) used to measure phase (phase to OPD conversion)\n",
    "aos_model = aos_model.insert_layer(layer=(\"ZygoPhaseScreen\", dl.BasisLayer(basis=jnp.array([resized_masked_phase_screen*test_wl]),\\\n",
    "                                                                               coefficients=jnp.array([1]), as_phase=False)),index=1)\n",
    "print(aos_model, aos_model.aperture.coefficients)\n",
    "aos_psf = aos_model.propagate_mono(wavelength=green_laser_wl)\n",
    "\n",
    "\n",
    "# load final, optimised, model (using green, spider 180 - best residuals)\n",
    "final_model = optics_sp180.set(\"aperture.coefficients\", green_glued_coeffs)\n",
    "final_model = final_model.set(\"spider.rotation\", final_spider_angle[0])\n",
    "final_model = dl.Telescope(final_model, ('source', green_src))\n",
    "final_model = final_model.set(\"source.position\", np.asarray([final_pos[0][0],final_pos[0][1]]))\n",
    "print(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare phase screens (using red, spider 180)\n",
    "%matplotlib inline\n",
    "retrieved_basis_eval = np.tensordot(final_model.aperture.basis, final_model.aperture.coefficients, \\\n",
    "                                    axes=2 * (tuple(range(final_model.aperture.coefficients.ndim)),))\n",
    "\n",
    "green_sp180_data = imgs[0][0,:,:]\n",
    "psf_center_idx = np.unravel_index(np.argmax(green_sp180_data, axis=None), green_sp180_data.shape)\n",
    "green_sp180_data = green_sp180_data[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "                                    psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "mask = np.ones(green_sp180_data.shape)\n",
    "mask[green_sp180_data <= 255] = 0\n",
    "\n",
    "datapsfNorm = PowerNorm(gamma=0.2, vmin=green_sp180_data.min(), vmax=green_sp180_data.max())\n",
    "\n",
    "plt.figure(figsize=(25,12))\n",
    "plt.subplot(2,4,1)\n",
    "plt.imshow(green_sp180_data, norm=datapsfNorm)\n",
    "plt.colorbar()\n",
    "plt.title(\"Measured Data\")\n",
    "plt.subplot(2,4,2)\n",
    "model_psf = final_model.model()\n",
    "model_psf_remapped =  green_sp180_data.min() + ((green_sp180_data.max() - green_sp180_data.min())/(model_psf.max()-model_psf.min()))*(model_psf - model_psf.min()) # [0,1]\n",
    "psfNorm = PowerNorm(gamma=0.2, vmin=model_psf_remapped.min(), vmax=model_psf_remapped.max())\n",
    "plt.imshow(model_psf_remapped*mask, norm=psfNorm) \n",
    "plt.title(\"Retrieved Image\")\n",
    "plt.colorbar()\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "plt.subplot(2,4,3)\n",
    "model_psf_remapped =  green_sp180_data.min() + ((green_sp180_data.max() - green_sp180_data.min())/(model_psf.max()-model_psf.min()))*(model_psf - model_psf.min()) # [0,1]\n",
    "resid = green_sp180_data - model_psf_remapped\n",
    "resid_norm = PowerNorm(gamma=1, vmin=-np.max(np.abs(resid)), vmax=np.max(np.abs(resid)))\n",
    "plt.imshow(resid, norm=resid_norm, cmap='bwr')\n",
    "plt.title(\"Data - simulated\")\n",
    "plt.colorbar()\n",
    "plt.subplot(2,4,4)\n",
    "plt.imshow(retrieved_basis_eval)\n",
    "plt.title(\"Retrieved Basis\")\n",
    "plt.colorbar(label=\"OPD (m)\")\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "plt.subplot(2,4,5)\n",
    "plt.imshow(green_sp180_data, norm=datapsfNorm)\n",
    "plt.title(\"Measured Data\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2,4,6)\n",
    "psf_center_idx = np.unravel_index(np.argmax(aos_psf, axis=None), aos_psf.shape)\n",
    "aos_psf_cropped = aos_psf[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "                                    psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "aos_psf_cropped_remap =  green_sp180_data.min() + ((green_sp180_data.max() - green_sp180_data.min())/(aos_psf_cropped.max()-aos_psf_cropped.min()))*(aos_psf_cropped - aos_psf_cropped.min()) # [0,1]\n",
    "psfNorm = PowerNorm(gamma=0.2, vmin=aos_psf_cropped_remap.min(), vmax=aos_psf_cropped_remap.max())\n",
    "plt.imshow(aos_psf_cropped_remap*mask, norm=psfNorm)\n",
    "plt.title(\"Model Image - w AOS phase\")\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "plt.colorbar()\n",
    "plt.subplot(2,4,7)\n",
    "aos_psf_cropped_remap =  green_sp180_data.min() + ((green_sp180_data.max() - green_sp180_data.min())/(aos_psf_cropped.max()-aos_psf_cropped.min()))*(aos_psf_cropped - aos_psf_cropped.min()) # [0,1]\n",
    "resid = green_sp180_data - aos_psf_cropped_remap\n",
    "resid_norm = PowerNorm(gamma=1, vmin=-np.max(np.abs(resid)), vmax=np.max(np.abs(resid)))\n",
    "plt.imshow(resid, norm=resid_norm, cmap='bwr')\n",
    "plt.title(\"Data - simulated\")\n",
    "plt.colorbar()\n",
    "plt.subplot(2,4,8)\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "plt.imshow(aos_model.ZygoPhaseScreen.eval_basis())\n",
    "plt.title(\"AOS Measured Phase\")\n",
    "plt.colorbar(label=\"OPD (m)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
