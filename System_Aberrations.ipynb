{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOLIMAN Pupil Gluing Analysis: Preliminary Measurements\n",
    "\n",
    "**Aim:**  \n",
    "To determine:\n",
    "1. The optical aberrations induced by the lab setup for pupil testing (consisting of 2 OAPs)\n",
    "2. The intensity distribution ouput from the optical fiber output for later modelling\n",
    "\n",
    "If we can show these aberrations are static over a long enough period of time (> 30min) then we can confidently remove them from the phase retrieval analysis of the later measurements (glued vs non-glued).\n",
    "\n",
    "We have chosen to place a spider mask (necessary asymmetry) within the collimated beam to characterise these aberrations via phase retrieval (thank u differentiable modelling/dLux ðŸ’–)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dLux as dl\n",
    "import dLux.utils as dlu\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jax.random as jr\n",
    "import jax.scipy as jsp\n",
    "from jax import vmap  \n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_debug_nans\", False)\n",
    "jax.config.update('jax_disable_jit', False)\n",
    "\n",
    "import zodiax as zdx\n",
    "import optax\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.filters import window\n",
    "import skimage as ski\n",
    "from skimage.transform import resize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import PowerNorm\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'inferno'\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"image.origin\"] = 'upper'\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "plt.rcParams[\"axes.titlesize\"] = 18\n",
    "plt.rcParams[\"figure.titlesize\"] = 18\n",
    "plt.rcParams[\"axes.labelsize\"] = 15\n",
    "\n",
    "data_dir = \"/import/morgana2/gpir9156/toliman/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Physical Parameters ---------------------------------------------------------------------#\n",
    "aperture_npix = 512           # Number of pixels across the aperture\n",
    "aperture_diameter = 122e-3# for spider cap 126e-3    # (m)\n",
    "spider_width = 20e-3          # Spider width (m)\n",
    "spider_angle =270             # Spider angle (degrees), clockwise, 0 is spider pointing vertically up\n",
    "coords = dlu.pixel_coords(npixels=aperture_npix, diameter=aperture_diameter)\n",
    "circle = dlu.circle(coords=coords, radius=aperture_diameter/2) \n",
    "\n",
    "# Observations wavelengths (bandpass of 530-640nm)\n",
    "red_laser_wl =  635e-09  # for laser data\n",
    "green_laser_wl = 520e-09  # for laser data\n",
    "wf_npixels = aperture_npix  # Number of pixels across the wavefront\n",
    "wf_diam = aperture_diameter             # Diameter of initial wavefront to propagate wavefront (m)\n",
    "\n",
    "# Dtector parameters (BFS-U3-200S6-BD)\n",
    "BFS_px_sep = 2.4e-6 *1e3        # pixel separation (mm)\n",
    "f_det = 1300#1350                    # Focal length from OAP2 to detector (mm) \n",
    "px_ang_sep = 2*np.arctan( (BFS_px_sep/2)/f_det ) # angular sep between pixels (rad)\n",
    "\n",
    "# Simulated Detector\n",
    "psf_npix = 28                 # Number of pixels along one dim of the PSF\n",
    "psf_hlf_sz = int(psf_npix/2)  \n",
    "oversample = 1                 # Oversampling factor for the PSF\n",
    "psf_pixel_scale = dlu.rad2arcsec(px_ang_sep) # arcsec (to match detector plate scale) 80e-4 \n",
    "\n",
    "# Detector response (gamma curve)\n",
    "alpha = np.load(data_dir+\"detector/80us_detector/ALPHA_norm.npy\")\n",
    "beta = np.load(data_dir+\"detector/80us_detector/BETA_norm.npy\")\n",
    "gamma = np.load(data_dir+\"detector/80us_detector/GAMMA_norm.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in some âœ¨realâœ¨ data ðŸŒˆ\n",
    "- Check intensity distribution across pupil first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = data_dir + \"intensity/15_07_intensity_image_test.png\"\n",
    "# data = imread(fname, as_gray=True) \n",
    "# data= np.flip(data) # det saves images fliiped upside down\n",
    "# manual_lim = [1363,4203,386,3214]\n",
    "# data = data[manual_lim[2]:manual_lim[3], manual_lim[0]:manual_lim[1]]\n",
    "# data = (data - data.min())/(data.max()-data.min())\n",
    "\n",
    "# intensity_dist = np.load(data_dir+\"spider/11_11_2024/RHCP_intensity_img_80us_0gain_img_stack_batch_0.npy\")[0,:]\n",
    "intensity_dist = np.load(data_dir+\"intensity/15_07_intensity_img_stack.npy\")[0,:]\n",
    "intensity_dist = np.flipud(intensity_dist)\n",
    "\n",
    "\n",
    "manual_cen = [1870,2770] # [row, col]\n",
    "hlf_sz = 1400      \n",
    "intensity_dist = intensity_dist[manual_cen[0]-hlf_sz:manual_cen[0]+hlf_sz, manual_cen[1]-hlf_sz:manual_cen[1]+hlf_sz]\n",
    "intensity_dist = 0 + (1-0)/(intensity_dist.max()- intensity_dist.min())*(intensity_dist - intensity_dist.min())\n",
    "\n",
    "# blurred = ski.filters.gaussian(data, sigma=(120, 120))\n",
    "blurred = ski.filters.gaussian(intensity_dist, sigma=(120, 120))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(intensity_dist)\n",
    "plt.title(\"Data - pre-focus\")\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(blurred)\n",
    "plt.title(\"Data blurred\")\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,3)\n",
    "intensity_dist = resize(blurred, (aperture_npix, aperture_npix))\n",
    "intensity_dist = (intensity_dist - intensity_dist.min())/(intensity_dist.max()-intensity_dist.min()) # re-map from 0-1\n",
    "plt.title(\"Blurred re-sized\")\n",
    "plt.imshow(intensity_dist)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dLux class for this gauss transmissive layer\n",
    "# maybe modelling a source object would be easier?\n",
    "class GaussTransmissiveLayer(dl.layers.optical_layers.TransmissiveLayer):\n",
    "    \"\"\"\n",
    "        Inherits from dl.layers.TransmissiveLayer and allows for\n",
    "        a Gaussian transmissive layer to be simulated. Useful for \n",
    "        simulating a wavefront that is not uniform in intensity.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        transmission: Array\n",
    "            The Array of transmission values to be applied to the input wavefront.\n",
    "        gaussian_param: Array = [var_x, var_y], shape (2,)\n",
    "            The parameters defining the 2D Gaussian to be applied to the input wavefront.\n",
    "            Where:\n",
    "            var_x, var_y = float\n",
    "                Variance in the x and y directions (m)\n",
    "            Center of Gaussian is given by position of Pointsource\n",
    "        pixel_coords : Array\n",
    "            3D array of pixel coordinates over which gaussian is defined in the shape\n",
    "            (2, npix, npix) where npix is the number of pixels across one dimension of the\n",
    "            each 2D array (one for X and Y).\n",
    "        det_npix: int\n",
    "            Number of pixels across the detector.\n",
    "        psf_pixel_scale: float\n",
    "            Pixel scale of the detector (arcsec/px).\n",
    "        normalise: bool\n",
    "            Whether to normalise the wavefront after passing through the optic.\n",
    "    \"\"\"\n",
    "    X: jnp.array\n",
    "    Y: jnp.array\n",
    "\n",
    "    gauss_param: jnp.array\n",
    "    point_source: dl.PointSources\n",
    "    det_npix: int\n",
    "    pp_npix: int # pupil-plane pixel size\n",
    "    psf_pixel_scale: float\n",
    "    pixel_size: float\n",
    "\n",
    "    def __init__(\n",
    "        self: dl.layers.optical_layers.OpticalLayer,\n",
    "        point_source: dl.PointSources,\n",
    "        gaussian_param: jnp.array,\n",
    "        pixel_coords: jnp.array,\n",
    "        det_npix: int,\n",
    "        psf_pixel_scale: float,\n",
    "        normalise: bool = False,\n",
    "    ):\n",
    "        self.X, self.Y = pixel_coords\n",
    "        assert gaussian_param.shape == (2,), \"Gaussian parameters must be of shape (2,) in form [var_x, var_y] \"\n",
    "        self.pp_npix = self.X.shape[0]\n",
    "        self.gauss_param = gaussian_param\n",
    "        self.point_source = point_source\n",
    "\n",
    "        self.det_npix = det_npix\n",
    "        self.psf_pixel_scale = psf_pixel_scale\n",
    "        self.pixel_size = jnp.abs(pixel_coords[0,0,1] - pixel_coords[0,0,0]) #distance between adjacent px\n",
    "\n",
    "        trans = self.get_transmission()\n",
    "\n",
    "        super().__init__(transmission=trans, normalise=normalise)\n",
    "\n",
    "    def get_transmission(self):\n",
    "        # angular position to number of pixels across detector\n",
    "        pos = self.point_source.position[0]\n",
    "        x_0_px = dlu.rad2arcsec(pos[0]) / self.psf_pixel_scale \n",
    "        y_0_px = dlu.rad2arcsec(pos[1]) / self.psf_pixel_scale \n",
    "\n",
    "        # convert to number of pixels across pupil plane\n",
    "        x_0_px *= self.pp_npix / self.det_npix\n",
    "        y_0_px *= self.pp_npix / self.det_npix \n",
    "\n",
    "        # convert to linear distance (m) across pupil plane\n",
    "        x_0 = x_0_px*self.pixel_size\n",
    "        y_0 = y_0_px*self.pixel_size\n",
    "\n",
    "        # x_0 *= -1 #dLux conventions or Positon\n",
    "        # y_0 *= -1\n",
    "\n",
    "        z = jnp.exp(-(self.X - x_0)**2/(2*self.gauss_param[0]**2) - (self.Y - y_0)**2/(2*self.gauss_param[1]**2))\n",
    "\n",
    "        return z \n",
    "\n",
    "    def apply(self: dl.layers.optical_layers.OpticalLayer, wavefront: dl.wavefronts.Wavefront) -> dl.wavefronts.Wavefront:\n",
    "        \"\"\"\n",
    "        Applies the layer to the wavefront.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        wavefront : Wavefront\n",
    "            The wavefront to operate on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        wavefront : Wavefront\n",
    "            The transformed wavefront.\n",
    "        \"\"\"\n",
    "        wavefront *= self.get_transmission()\n",
    "        if self.normalise:\n",
    "            wavefront = wavefront.normalise()\n",
    "        return wavefront\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define non-linear detector response\n",
    "class ApplyBFSPixelResponse(dl.layers.detector_layers.DetectorLayer):\n",
    "    \"\"\"\n",
    "    Applies a pixel response array to the input psf, via a multiplication. This can be\n",
    "    used to model variations in the inter and intra-pixel sensitivity variations common\n",
    "    to most detectors.\n",
    "\n",
    "    We have characterised the BFS detector to have a gamma curve gain response per px.\n",
    "\n",
    "    ??? abstract \"UML\"\n",
    "        ![UML](../../assets/uml/ApplyPixelResponse.png)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pixel_response : Array\n",
    "        The pixel_response to apply to the input psf in the form of gamma fn coeffs:\n",
    "        measured_intensity = alpha + beta*real_intensity^gamma\n",
    "        with    pixel_response[0] = alpha\n",
    "                pixel_response[1] = beta\n",
    "                pixel_response[2] = gamma\n",
    "    luminance_min, luminance_max : float\n",
    "        The minimum and maximum input luminance values for the pixel response (with which the \n",
    "        gamma curve was characterised).\n",
    "    intensity_min, intensity_max : float\n",
    "        The minimum and maximum output intensity values for the pixel response (with which the \n",
    "        gamma curve was characterised).\n",
    "    \"\"\"\n",
    "\n",
    "    pixel_response: jnp.array\n",
    "    x_intercepts: jnp.array\n",
    "    # luminance_min: float\n",
    "    # luminance_max: float\n",
    "    # intensity_min: float\n",
    "    # intensity_max: jnp.float64\n",
    "\n",
    "    def __init__(self: dl.layers.detector_layers.DetectorLayer, \n",
    "                 pixel_response: jnp.array,\n",
    "                # luminance_min: float,\n",
    "                # luminance_max: float,\n",
    "                # intensity_min: float,\n",
    "                # intensity_max: float,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        pixel_response : Array\n",
    "            The pixel_response to apply to the input psf. Must be a 2-dimensional array\n",
    "            equal to size of the psf at time of application.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # assert pixel_response.shape[0] == 3, \"Pixel response must contain 3 matrices corresponding to gamma fn coeffs\"\n",
    "        # assert luminance_max > luminance_min, \"luminance_max must be greater than luminance_min\"\n",
    "        # assert intensity_max > intensity_min, \"intensity_max must be greater than intensity_min\"\n",
    "\n",
    "        self.pixel_response = jnp.asarray(pixel_response, dtype=float)\n",
    "        self.x_intercepts = self.calc_x_intercepts()\n",
    "        # self.luminance_min = jnp.array(luminance_min, dtype=float)\n",
    "        # self.luminance_max = jnp.array(luminance_max, dtype=float)\n",
    "        # self.intensity_min = jnp.array(intensity_min, dtype=float)\n",
    "        # self.intensity_max = jnp.array(intensity_max, dtype=float)\n",
    "\n",
    "    def calc_x_intercepts(self):\n",
    "        \"\"\"\n",
    "            Calculate the x-intercepts of the gamma function, for input intensity values\n",
    "            that result in curve below x-axis. Think this is just a precision artefact.\n",
    "        \"\"\"\n",
    "        x_int = jnp.power((-self.pixel_response[0] / self.pixel_response[1]), 1/self.pixel_response[2])\n",
    "        \n",
    "        return x_int\n",
    "\n",
    "\n",
    "    def apply(self: dl.layers.detector_layers.DetectorLayer, psf: dl.PSF) -> dl.PSF:\n",
    "        \"\"\"\n",
    "        Applies the layer to the PSF.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psf : PSF\n",
    "            The psf to operate on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        psf : PSF\n",
    "            The transformed psf.\n",
    "\n",
    "        \"\"\"\n",
    "        psf_array = jnp.asarray(psf.data, dtype=float)\n",
    "        # # assert psf_array.shape == self.pixel_response[0].shape and\\\n",
    "        # # psf_array.shape == self.pixel_response[1].shape and \\\n",
    "        # # psf_array.shape == self.pixel_response[2].shape, \"PSF and pixel response must have same shape\"\n",
    "\n",
    "        # # remap to [luminance_min, luminance_max] for measured gamma fn \n",
    "        # orig_min, orig_max = psf_array.min(), psf_array.max()\n",
    "        # # print(orig_min, orig_max)\n",
    "        # # data_remapped = psf_array #0.0 + ((1.0 - 0.0)/(orig_max-orig_min))*(psf_array - orig_min)\n",
    "\n",
    "        # # # t1 = jnp.isnan(orig_max-orig_min)\n",
    "        # # # t2 = jnp.isnan(psf_array - orig_min)\n",
    "        # # # print(t1.sum(), t2.sum())\n",
    "        # # # print(\"data remapped max and min:\", data_remapped.max(), data_remapped.min())\n",
    "\n",
    "        # power = psf_array**self.pixel_response[2] # think jnp.power() is causing nan grads\n",
    "        # measured_psf = self.pixel_response[0] + self.pixel_response[1]*power #element-wise power\n",
    "\n",
    "        # # neg_mask = measured_psf < 0.0\n",
    "        # # measured_psf = measured_psf*jnp.invert(neg_mask) + psf_array*neg_mask # take orig values if neg\n",
    "\n",
    "\n",
    "        # # print(\"coeffs: {}, {},{}, input intensity: {} output intensity: {}\".format(self.pixel_response[0][0,0],self.pixel_response[1][0,0],self.pixel_response[2][0,0], psf_array[0,0], measured_psf[0,0]))\n",
    "        \n",
    "        # # # for some reason this is not guaranteed to be in the range of [intensity_min, intensity_max]?? fns calc should be in range\n",
    "        # # # either interpet this as 0 pixel or remap range? \n",
    "        # # Because of floating point precision limitations - very small values show curve below x-axis. Remap to fix this.\n",
    "        # measured_psf = orig_min+ ((1.0 - orig_min)/(measured_psf.max()-measured_psf.min()))*(measured_psf - measured_psf.min())\n",
    "        # # # print(measured_psf.max(), measured_psf.min())\n",
    "\n",
    "        # # # print(self.pixel_response[0][0,0],self.pixel_response[1][0,0],self.pixel_response[2][0,0])\n",
    "        # # # reamap back to original range (in range of [intensity_min, intensity_max] currently)\n",
    "        # # measured_psf = orig_min + ((orig_max - orig_min)/(1.0 - 0.0))*(measured_psf - 0.0)\n",
    "\n",
    "        # # # psf_obj = dl.PSF(data = data_remapped, pixel_scale = psf.pixel_scale)\n",
    "        # # measured_psf = measured_psf.at[measured_psf < 0.0].set(0.0001)\n",
    "        # # psf_obj = dl.PSF(data = jnp.abs(measured_psf), pixel_scale = psf.pixel_scale)\n",
    "        # psf_obj = dl.PSF(data = (measured_psf), pixel_scale = psf.pixel_scale)\n",
    "\n",
    "\n",
    "        psf_obj = dl.PSF(data = psf_array**(1/1), pixel_scale = psf.pixel_scale)\n",
    "\n",
    "\n",
    "        return psf_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotatingTransmissiveLayer(dl.layers.TransmissiveLayer):\n",
    "    \"\"\"\n",
    "    Base class to hold transmissive layers imbuing them with a transmission and\n",
    "    normalise parameter.\n",
    "\n",
    "    ??? abstract \"UML\"\n",
    "        ![UML](../../assets/uml/TransmissiveLayer.png)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    transmission: Array\n",
    "        The Array of transmission values to be applied to the input wavefront.\n",
    "    normalise: bool\n",
    "        Whether to normalise the wavefront after passing through the optic.\n",
    "    rotation: Array([float])\n",
    "        Single value for rotation of transmissive layer (radians). \n",
    "        Array of shape (1,) (zodiax artefact requires array to \n",
    "        optimise on single value). Rotation applied CW\n",
    "    \"\"\"\n",
    "\n",
    "    rotation: np.array\n",
    "\n",
    "    def __init__(\n",
    "        self: dl.layers.optical_layers.OpticalLayer,\n",
    "        transmission: np.array = None,\n",
    "        normalise: bool = False,\n",
    "        rotation: np.array = np.array([0.0]),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        transmission: Array = None\n",
    "            The array of transmission values to be applied to the input wavefront.\n",
    "        normalise : bool = False\n",
    "            Whether to normalise the wavefront after passing through the optic.\n",
    "        rotation: Array([float])\n",
    "            Single value for rotation of transmissive layer (radians). \n",
    "            Array of shape (1,) (zodiax artefact requires array to \n",
    "            optimise on single value). Rotation applied CW\n",
    "        \"\"\"\n",
    "        self.rotation = rotation\n",
    "        super().__init__(transmission=transmission, normalise=normalise,**kwargs)\n",
    "\n",
    "    def apply(self: dl.layers.optical_layers.OpticalLayer, wavefront: dl.wavefronts.Wavefront) -> dl.wavefronts.Wavefront:\n",
    "        \"\"\"\n",
    "        Applies the layer to the wavefront.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        wavefront : Wavefront\n",
    "            The wavefront to operate on.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        wavefront : Wavefront\n",
    "            The transformed wavefront.\n",
    "        \"\"\"\n",
    "        wavefront *= dlu.rotate(self.transmission, self.rotation) \n",
    "        if self.normalise:\n",
    "            wavefront = wavefront.normalise()\n",
    "        return wavefront\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zernike aberrations\n",
    "zernike_indicies = jnp.arange(4, 15) # up to 10th noll idxs (excluding piston)\n",
    "coeffs = jnp.zeros(zernike_indicies.shape)\n",
    "basis = dlu.zernike_basis(js=zernike_indicies, coordinates=coords, diameter=aperture_diameter)\n",
    "\n",
    "# Using PointSources instead of single PointSource object to overcome float grad issue when solving for flux\n",
    "red_src = dl.PointSources(wavelengths=[red_laser_wl], flux =jnp.asarray([1e8],dtype=float))\n",
    "\n",
    "spider_angles = [180, 270] # 0deg is spider pointing vertically up (if looking at 'lower' origin), rotates CW from 0deg\n",
    "optical_systems = []\n",
    "transmissions = []\n",
    "for i in range(len(spider_angles)):\n",
    "    spider = dlu.spider(coords=coords, width=spider_width, angles=[spider_angles[i]])\n",
    "    transmission = dlu.combine([circle, spider])\n",
    "\n",
    "    layers = [\n",
    "        ('intensity', dl.layers.TransmissiveLayer(transmission=intensity_dist, normalise=False)),\n",
    "        ('spider', RotatingTransmissiveLayer(transmission=transmission, normalise=False)),\n",
    "        ('aperture', dl.layers.BasisOptic(basis=basis, coefficients=coeffs, transmission=None, normalise=False)),\n",
    "    ]\n",
    "\n",
    "    optics = dl.AngularOpticalSystem(wf_npixels = wf_npixels, \n",
    "                                diameter=wf_diam, \n",
    "                                layers=layers, \n",
    "                                psf_npixels=psf_npix, \n",
    "                                psf_pixel_scale=psf_pixel_scale,\n",
    "                                oversample=oversample)\n",
    "\n",
    "    optical_systems.append(optics)\n",
    "    transmissions.append(transmission)\n",
    "\n",
    "# Construct Optics\n",
    "optics_sp270  = optical_systems[1]\n",
    "optics_sp180 = optical_systems[0]\n",
    "sp270_trans = transmissions[1]\n",
    "sp180_trans = transmissions[0]\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(sp180_trans*optics_sp180.intensity.transmission)\n",
    "plt.title(\"180deg\")\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(sp270_trans*optics_sp270.intensity.transmission)\n",
    "plt.title(\"270deg\")\n",
    "plt.colorbar()\n",
    "\n",
    "# ----------------------------------------------------------------------------------#\n",
    "# # Create detector layer to characterise non-linear response of BFS det \n",
    "# row_start, col_start = 1676, 2532 # Start coord of window on det where data was taken\n",
    "# row_len, col_len = 86, 76   # window size of recorded data\n",
    "# psf_hlf_sz = 14             # half window sz of cropped data\n",
    "\n",
    "# # load in response curve matrices (form of Gamma fn)\n",
    "# alpha = np.load(\"data/80us_detector/ALPHA_norm.npy\")[row_start:row_start+row_len, col_start:col_start+col_len]\n",
    "# beta = np.load(\"data/80us_detector/BETA_norm.npy\")[row_start:row_start+row_len, col_start:col_start+col_len]\n",
    "# gamma = np.load(\"data/80us_detector/GAMMA_norm.npy\")[row_start:row_start+row_len, col_start:col_start+col_len]\n",
    "\n",
    "# # Crop, anchored on middle (cropped data will be based on max values, but this occurs close enough to the\n",
    "# # middle to estimate here. + detector response is fairly homogenous over small window)\n",
    "# middle_row, middle_col = int(alpha.shape[0]/2), int(alpha.shape[1]/2)\n",
    "# cropped_alpha = alpha[middle_row-psf_hlf_sz:middle_row+psf_hlf_sz, middle_col-psf_hlf_sz:middle_col+psf_hlf_sz]\n",
    "# cropped_beta = beta[middle_row-psf_hlf_sz:middle_row+psf_hlf_sz, middle_col-psf_hlf_sz:middle_col+psf_hlf_sz]\n",
    "# cropped_gamma = gamma[middle_row-psf_hlf_sz:middle_row+psf_hlf_sz, middle_col-psf_hlf_sz:middle_col+psf_hlf_sz]\n",
    "\n",
    "# pixel_response = jnp.array([cropped_alpha, cropped_beta, cropped_gamma])\n",
    "# detector = dl.LayeredDetector([ApplyBFSPixelResponse(pixel_response=pixel_response)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss fn's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    'aperture.coefficients',\n",
    "    'spider.rotation',\n",
    "    'source.position',\n",
    "    'source.flux', \n",
    "    ]\n",
    "optimisers = [\n",
    "            optax.adam(learning_rate=1e-9),\n",
    "            optax.adam(learning_rate=1e-2),\n",
    "            optax.adam(learning_rate=1e-7),\n",
    "            optax.adam(learning_rate=1e6),\n",
    "              ]\n",
    "@zdx.filter_jit\n",
    "@zdx.filter_value_and_grad(params)\n",
    "def loss_fn_gaussian(model, data):\n",
    "\n",
    "    simu_psf = model.model()\n",
    "\n",
    "    uncertainty = 0.1 # 10% err per pix TODO try increasing\n",
    "\n",
    "    loss = -jsp.stats.norm.logpdf(x=simu_psf, loc=data, scale=data*uncertainty).sum()\n",
    "\n",
    "    return loss\n",
    "@zdx.filter_jit\n",
    "@zdx.filter_value_and_grad(params)\n",
    "def loss_fn_poisson(model, data):\n",
    "\n",
    "    simu_psf = model.model()\n",
    "\n",
    "    loss = -jsp.stats.poisson.logpmf(k=simu_psf, mu=data).sum()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets loop this\n",
    "\n",
    "Fit on both models simultaneously (taking avg gradient from likelihood fn), using ~30 frames to get avgs of coeffs, with mean as SEMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location on detector \n",
    "row_start, col_start = 1562, 2572 # Start coord of window on det where data was taken\n",
    "row_len, col_len = 80, 60   # window size of recorded data\n",
    "\n",
    "alpha_cropped = alpha[row_start:row_start+row_len, col_start:col_start+col_len]\n",
    "beta_cropped = beta[row_start:row_start+row_len, col_start:col_start+col_len]\n",
    "gamma_cropped = gamma[row_start:row_start+row_len, col_start:col_start+col_len]\n",
    "\n",
    "img_fnames = [\n",
    "    \"spider/17_12_0deg_img_80us_0gain_img_stack_batch_0.npy\",\n",
    "    \"spider/17_12_90deg_img_69us_0gain_img_stack_batch_0.npy\",\n",
    "]\n",
    "labels = [\n",
    "    \"0deg\",\n",
    "    \"90deg\",\n",
    "]\n",
    "\n",
    "bckgnd_fnames = [\n",
    "    \"spider/17_12_0deg_img_bckgnd_80us_0gain_img_stack_batch_0.npy\",\n",
    "    \"spider/17_12_90deg_img_bckgnd_80us_0gain_img_stack_batch_0.npy\",\n",
    "]\n",
    "\n",
    "imgs = np.asarray([np.flipud(np.load(data_dir+img_fname)) for img_fname in img_fnames])\n",
    "\n",
    "bckgnds = np.asarray([np.mean(np.flipud(np.load(data_dir+bckgnd_fname))) for bckgnd_fname in bckgnd_fnames])  # single bckgnd frame avg'd\n",
    "\n",
    "n_frames = 30   #number of frames to perform phase retrieval on\n",
    "step_sz = int(imgs[0].shape[0]/n_frames) # step size for frames (equally spaced frame grabs)\n",
    "print(imgs[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store final loss and coefficients for each frame, for each configuration : [0, 90, combined]\n",
    "final_loss_configs = [[], [], []]\n",
    "retrieved_coeffs_configs = [[], [], []]\n",
    "final_pos_configs = [[], [], []]\n",
    "final_flux_configs = [[], [], []]\n",
    "frames = []\n",
    "\n",
    "dir_ = \"data/spider/retrieval_results/17_12/\" # save png's of phase retrieval results\n",
    "dir_configs = [dir_ + \"0deg/\", dir_ + \"90deg/\", dir_ + \"combined/\"]\n",
    "\n",
    "for i in range(n_frames):\n",
    "    ###---------------------------- Re-init models ----------------------------###\n",
    "    instrument_sp180 = dl.Telescope(optics_sp180, ('source', red_src))\n",
    "    instrument_sp270 = dl.Telescope(optics_sp270, ('source', red_src))\n",
    "\n",
    "    sim_psfs = [instrument_sp180.model(), instrument_sp270.model()]\n",
    "    norm_psfs = [PowerNorm(0.2, vmax=sim_psf.max(), vmin=sim_psf.min()) for sim_psf in sim_psfs]    \n",
    "\n",
    "    ###------------------------------- Get Frame -----------------------------###\n",
    "    frame_n = i*step_sz-1 if i != 0 else 0\n",
    "    frames.append(frame_n)\n",
    "    data_sp180 = imgs[0][frame_n,:,:] # assuming only two spider configs, given in order of increasing angle\n",
    "    data_sp270 = imgs[1][frame_n,:,:]\n",
    "    data_list = [data_sp180, data_sp270]\n",
    "    scaled_data_list = []\n",
    "    for j in range(len(data_list)):\n",
    "        data = data_list[j]\n",
    "        if len(bckgnds) > 0:\n",
    "            data = data - bckgnds[j]\n",
    "\n",
    "        # Reverse-model detector response\n",
    "        data_remapped = 0.0 + ((1.0 - 0.0)/(data.max()-data.min()))*(data - data.min()) #[0,1] otput range\n",
    "        data = np.power((data_remapped-np.flipud(alpha_cropped))/np.flipud(beta_cropped), (1/np.flipud(gamma_cropped))) # flip vertically\n",
    "\n",
    "        # Scale intensity\n",
    "        current_range = data.max() - data.min()\n",
    "        new_range = sim_psfs[j].max() - sim_psfs[j].min()\n",
    "        scaled_data = ( (data - data.min()) * new_range )/current_range + sim_psfs[j].min()\n",
    "\n",
    "        psf_center_idx = np.unravel_index(np.argmax(scaled_data, axis=None), scaled_data.shape)\n",
    "        scaled_data = scaled_data[psf_center_idx[0]-psf_hlf_sz:psf_center_idx[0]+psf_hlf_sz,\n",
    "                                    psf_center_idx[1]-psf_hlf_sz:psf_center_idx[1]+psf_hlf_sz]\n",
    "        \n",
    "        scaled_data_list.append(scaled_data)\n",
    "\n",
    "\n",
    "    ###------------------------------- Phase Retrieval -----------------------------###\n",
    "    configs = [instrument_sp180, instrument_sp270] # model configurations to optimise upon\n",
    "\n",
    "    # # Solve for individual configs firstly \n",
    "    # for k, instrument in enumerate(configs):\n",
    "    #     scaled_data = scaled_data_list[k]\n",
    "\n",
    "    #     optim, opt_state = zdx.get_optimiser(instrument, params, optimisers)\n",
    "\n",
    "    #     progress_bar = tqdm(range(5000), desc='Loss: ')\n",
    "\n",
    "    #     # Run optimisation loop \n",
    "    #     net_losses, Coeffs, Positions, Fluxes= [],[],[],[]\n",
    "    #     for j in progress_bar:\n",
    "    #         poiss_loss, poiss_grads = loss_fn_poisson(model = instrument, data = scaled_data)\n",
    "\n",
    "    #         updates, opt_state = optim.update(poiss_grads, opt_state)\n",
    "    #         instrument = zdx.apply_updates(instrument, updates) \n",
    "\n",
    "    #         net_losses.append(poiss_loss)\n",
    "    #         Fluxes.append(instrument.source.flux)\n",
    "    #         Coeffs.append(instrument.aperture.coefficients)\n",
    "    #         Positions.append(instrument.source.position)\n",
    "\n",
    "    #         progress_bar.set_postfix({'Loss': poiss_loss, 'Frame': frame_n})\n",
    "\n",
    "    #     final_loss_configs[k].append(net_losses[-1])\n",
    "    #     retrieved_coeffs_configs[k].append(instrument.aperture.coefficients)\n",
    "    #     final_pos_configs[k].append(instrument.source.position[0])\n",
    "    #     final_flux_configs[k].append(instrument.source.flux[0])\n",
    "\n",
    "\n",
    "    #     ###---------------------------------- Plotting ---------------------------------###\n",
    "    #     plt.figure(figsize=(6,12))\n",
    "    #     plt.subplot(4,2,1)\n",
    "    #     plt.plot(np.asarray(Positions)[:,0,0], label=\"Position X\")\n",
    "    #     plt.plot(np.asarray(Positions)[:,0,1], label=\"Position Y\")\n",
    "    #     plt.title(\"Position\")\n",
    "    #     plt.legend()\n",
    "    #     plt.subplot(4,2,2)\n",
    "    #     arr_coeffs = np.asarray(Coeffs)\n",
    "    #     for l in range(len(Coeffs[0])):\n",
    "    #         label = \"Coeff \" + str(zernike_indicies[l])\n",
    "    #         plt.plot(arr_coeffs[:,l], label=label)\n",
    "    #     plt.legend()\n",
    "    #     plt.subplot(4,2,3)\n",
    "    #     plt.plot(np.asarray(Fluxes))\n",
    "    #     plt.title(\"Flux\")\n",
    "    #     plt.subplot(4,2,4)\n",
    "    #     plt.plot(np.array(net_losses))\n",
    "    #     ax = plt.gca()\n",
    "    #     ax.set_title(\"Training History\")\n",
    "    #     ax.set_xlabel(\"Training Epoch\")\n",
    "    #     ax.set_ylabel(\"Poisson Log-Likelihood\")\n",
    "\n",
    "\n",
    "    #     plt.subplot(4,2,5)\n",
    "    #     norm_psf = PowerNorm(0.2, vmax=scaled_data.max(), vmin=scaled_data.min())\n",
    "    #     plt.imshow(scaled_data, norm=norm_psf)\n",
    "    #     plt.colorbar()\n",
    "    #     plt.title('Data')\n",
    "\n",
    "    #     plt.subplot(4,2,6)\n",
    "    #     model_psf = instrument.model()\n",
    "    #     current_range = model_psf.max() - model_psf.min()\n",
    "    #     new_range = scaled_data.max() - scaled_data.min()\n",
    "    #     model_psf = ( (model_psf - model_psf.min()) * new_range )/current_range + scaled_data.min()\n",
    "    #     norm_psf = PowerNorm(0.2, vmax=model_psf.max(), vmin=model_psf.min())\n",
    "    #     mask = np.ones(scaled_data.shape)\n",
    "    #     mask[scaled_data < 0.01] = 0\n",
    "    #     plt.imshow(model_psf*mask, norm=norm_psf)\n",
    "    #     plt.title('Model')\n",
    "    #     plt.colorbar()\n",
    "\n",
    "    #     plt.subplot(4,2,7)\n",
    "    #     resid = scaled_data - model_psf\n",
    "    #     plt.imshow(resid, cmap='bwr', vmax = np.abs(resid).max(), vmin = -np.abs(resid).max())\n",
    "    #     plt.colorbar()\n",
    "    #     plt.title('Residuals')\n",
    "\n",
    "    #     plt.subplot(4,2,8)\n",
    "    #     opd = instrument.aperture.eval_basis()\n",
    "    #     trans = instrument.aperture.transmission\n",
    "    #     plt.imshow(opd*trans, cmap='viridis')\n",
    "    #     plt.title('Retrieved Aberrations')\n",
    "    #     plt.colorbar()\n",
    "    #     plt.tight_layout()\n",
    "\n",
    "    #     plt.savefig(dir_configs[k] + \"frame_\" + str(frame_n) + \".png\")\n",
    "    #     plt.close()\n",
    "\n",
    "\n",
    "    # -------------------------- Combined Configurations --------------------------#\n",
    "    # Now repeat for simultaneous solving between spider configs\n",
    "    # Pick either config here, all that matters is that they have the same coeff initialisation and structure\n",
    "    optim, opt_state = zdx.get_optimiser(instrument_sp180, params, optimisers)\n",
    "\n",
    "    progress_bar = tqdm(range(1000), desc='Loss: ')\n",
    "\n",
    "    # Run optimisation loop \n",
    "    net_losses, Coeffs, Positions_sp180, Positions_sp270, Fluxes, SpiderAngles= [],[],[],[],[],[]\n",
    "    for j in progress_bar:\n",
    "        grads = None\n",
    "        net_loss = 0\n",
    "        mean_coeff_grads, mean_position_grads, mean_flux_grads = None, None, None # to set initially in loop\n",
    "        pos_grads = [] # update positional gradients separately (right now I've centered data based on brightest pixel\n",
    "                        # but this is not a super robust method, so allow for source position difference)\n",
    "        spider_grads = [] # acc for spider err rotation independently between configs\n",
    "        for k in range(len(configs)):\n",
    "            loss, grads = loss_fn_poisson(model = configs[k], data = scaled_data_list[k])\n",
    "            net_loss += loss \n",
    "            \n",
    "            if k == 0:\n",
    "                mean_coeff_grads = grads.aperture.coefficients/len(configs)\n",
    "                mean_position_grads = grads.source.position/len(configs)\n",
    "                mean_flux_grads = grads.source.flux/len(configs)\n",
    "            else:\n",
    "                mean_coeff_grads += grads.aperture.coefficients/len(configs)\n",
    "                mean_position_grads += grads.source.position/len(configs)\n",
    "                mean_flux_grads += grads.source.flux/len(configs)\n",
    "\n",
    "            pos_grads.append(grads.source.position)\n",
    "            spider_grads.append(grads.spider.rotation)\n",
    "\n",
    "        grads = grads.set('aperture.coefficients', mean_coeff_grads)\n",
    "        grads = grads.set('source.position', mean_position_grads)\n",
    "        grads = grads.set('source.flux', mean_flux_grads)\n",
    "\n",
    "        for k in range(len(configs)):\n",
    "            grads = grads.set('source.position', pos_grads[k])\n",
    "            grads = grads.set('spider.rotation', spider_grads[k])\n",
    "            updates, opt_state = optim.update(grads, opt_state)\n",
    "            configs[k] = zdx.apply_updates(configs[k], updates)\n",
    "\n",
    "        net_losses.append(net_loss)\n",
    "        Fluxes.append(configs[0].source.flux)\n",
    "        Coeffs.append(configs[0].aperture.coefficients)\n",
    "        Positions_sp180.append(configs[0].source.position)\n",
    "        Positions_sp270.append(configs[1].source.position)\n",
    "        SpiderAngles.append([configs[0].spider.rotation, configs[1].spider.rotation])\n",
    "\n",
    "        progress_bar.set_postfix({'Loss (combined)': net_loss, 'Frame': frame_n})\n",
    "\n",
    "    final_loss_configs[-1].append(net_losses[-1])\n",
    "    retrieved_coeffs_configs[-1].append(configs[0].aperture.coefficients)\n",
    "    final_pos_configs[-1].append(configs[0].source.position[0])\n",
    "    final_flux_configs[-1].append(configs[0].source.flux[0])\n",
    "\n",
    "\n",
    "    ###---------------------------------- Plotting ---------------------------------###\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.subplot(5,3,1)\n",
    "    plt.plot(np.asarray(Positions_sp180)[:,0,0], label=\"Position X - sp180\")\n",
    "    plt.plot(np.asarray(Positions_sp180)[:,0,1], label=\"Position Y - sp180\")\n",
    "    plt.plot(np.asarray(Positions_sp270)[:,0,0], label=\"Position X - sp270\")\n",
    "    plt.plot(np.asarray(Positions_sp270)[:,0,1], label=\"Position Y - sp270\")\n",
    "    plt.title(\"Position\")\n",
    "    plt.legend()\n",
    "    plt.subplot(5,3,2)\n",
    "    arr_coeffs = np.asarray(Coeffs)\n",
    "    for l in range(len(Coeffs[0])):\n",
    "        label = \"Coeff \" + str(zernike_indicies[l])\n",
    "        plt.plot(arr_coeffs[:,l], label=label)\n",
    "    plt.legend()\n",
    "    plt.subplot(5,3,3)\n",
    "    plt.plot(np.asarray(Fluxes))\n",
    "    plt.title(\"Flux\")\n",
    "    plt.subplot(5,3,4)\n",
    "    plt.plot(np.array(net_losses))\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Training History\")\n",
    "    ax.set_xlabel(\"Training Epoch\")\n",
    "    ax.set_ylabel(\"Poisson Log-Likelihood\")\n",
    "\n",
    "    it = 5\n",
    "    for k, config in enumerate(configs):\n",
    "        plt.subplot(5,3,it)\n",
    "        opd = config.aperture.eval_basis()\n",
    "        trans = config.spider.transmission\n",
    "        plt.imshow(opd*trans, cmap='viridis')\n",
    "        plt.title('Retrieved Aberrations')\n",
    "        plt.colorbar()\n",
    "        it+=1\n",
    "\n",
    "    for k, config in enumerate(configs):\n",
    "        scaled_data = scaled_data_list[k]\n",
    "        instrument = config\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        norm_psf = PowerNorm(0.2, vmax=scaled_data.max(), vmin=scaled_data.min())\n",
    "        plt.imshow(scaled_data, norm=norm_psf)\n",
    "        plt.colorbar()\n",
    "        plt.title('Data')\n",
    "        it+=1\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        model_psf = instrument.model()\n",
    "        current_range = model_psf.max() - model_psf.min()\n",
    "        new_range = scaled_data.max() - scaled_data.min()\n",
    "        model_psf = ( (model_psf - model_psf.min()) * new_range )/current_range + scaled_data.min()\n",
    "        norm_psf = PowerNorm(0.2, vmax=model_psf.max(), vmin=model_psf.min())\n",
    "        mask = np.ones(scaled_data.shape)\n",
    "        mask[scaled_data < 0.01] = 0\n",
    "        plt.imshow(model_psf*mask, norm=norm_psf)\n",
    "        plt.title('Model')\n",
    "        plt.colorbar()\n",
    "        it+=1\n",
    "\n",
    "        plt.subplot(5,3,it)\n",
    "        resid = scaled_data - model_psf\n",
    "        plt.imshow(resid, cmap='bwr', vmax = np.abs(resid).max(), vmin = -np.abs(resid).max())\n",
    "        plt.colorbar()\n",
    "        plt.title('Residuals')\n",
    "        it+=1\n",
    "\n",
    "    plt.subplot(5,3,it)\n",
    "    spider_angles_arr = np.asarray(SpiderAngles)\n",
    "    plt.plot(spider_angles_arr[:,0], label=\"config0\")\n",
    "    plt.plot(spider_angles_arr[:,1], label=\"config0\")\n",
    "    plt.title(\"Spider Angle\")\n",
    "    plt.ylabel(\"Rotation (rad)\")\n",
    "\n",
    "    plt.savefig(dir_configs[-1] + \"frame_\" + str(frame_n) + \".png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well phase retrieval performed and only avg across \"succcessful\" retrievals\n",
    "# config_labels = ['0deg', '90deg', 'combined']\n",
    "# plt.figure(figsize=(15,5))\n",
    "# for i, final_loss in enumerate(final_loss_configs):\n",
    "#     plt.subplot(1,3,i+1)\n",
    "#     plt.scatter(frames, final_loss)\n",
    "#     plt.title(config_labels[i])\n",
    "#     plt.xlabel(\"Frame #\")\n",
    "#     plt.ylabel(\"Final Loss\")\n",
    "#     plt.grid()\n",
    "\n",
    "plt.scatter(frames, final_loss_configs[-1])\n",
    "plt.title('Combined Fitting')\n",
    "plt.xlabel(\"Frame #\")\n",
    "plt.ylabel(\"Final Loss\")\n",
    "plt.grid()\n",
    "\n",
    "# Threshold picked by eye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all 3 scenarios\n",
    "# retrieved_coeffs_configs = np.asarray(retrieved_coeffs_configs)\n",
    "# mean_coeffs_configs = np.mean(retrieved_coeffs_configs, axis=1)\n",
    "# print(mean_coeffs_configs.shape)\n",
    "# stdev = np.std(retrieved_coeffs_configs, axis=1)\n",
    "# SEMs = stdev/np.sqrt(n_frames)\n",
    "\n",
    "# fmt_markers = ['.','*','x']\n",
    "# plt.figure(figsize=(10,5))\n",
    "# for i, mean_coeffs_config in enumerate(mean_coeffs_configs):\n",
    "#     plt.errorbar(zernike_indicies, mean_coeffs_config, yerr=SEMs[i], fmt=fmt_markers[i], capsize=4, label=config_labels[i])\n",
    "\n",
    "# plt.title(\"Summary Coefficients: n = \" + str(n_frames)+ \" frames\")\n",
    "# plt.xlabel(\"Zernike Noll Idx\")\n",
    "# plt.ylabel(\"Mean Coefficient (m)\")\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "\n",
    "# For just combined fits\n",
    "retrieved_coeffs_configs_arr = np.asarray(retrieved_coeffs_configs[-1])\n",
    "mean_coeffs_configs = np.mean(retrieved_coeffs_configs_arr, axis=0)\n",
    "stdev = np.std(retrieved_coeffs_configs_arr, axis=0)\n",
    "SEMs = stdev/np.sqrt(n_frames)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.errorbar(zernike_indicies, mean_coeffs_configs, yerr=SEMs, fmt='x', capsize=4, label=\"Combined\")\n",
    "plt.title(\"Summary Coefficients: n = \" + str(n_frames)+ \" frames\")\n",
    "plt.xlabel(\"Zernike Noll Idx\")\n",
    "plt.ylabel(\"Mean Coefficient (m)\")\n",
    "plt.grid()\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store (combined)\n",
    "spider_coeff_dir = \"data/spider/retrieval_results/\"\n",
    "\n",
    "# np.save(spider_coeff_dir + \"mean_coeffs_0deg.npy\", mean_coeffs_configs[0])\n",
    "# np.save(spider_coeff_dir + \"mean_coeffs_90deg.npy\", mean_coeffs_configs[1])\n",
    "# np.save(spider_coeff_dir + \"mean_coeffs_combined.npy\", mean_coeffs_configs[2])\n",
    "\n",
    "# np.save(spider_coeff_dir + \"SEM_coeffs_0deg.npy\", SEMs[0])\n",
    "# np.save(spider_coeff_dir + \"SEM_coeffs_90deg.npy\", SEMs[1])\n",
    "# np.save(spider_coeff_dir + \"SEM_coeffs_combined.npy\", SEMs[2])\n",
    "\n",
    "np.save(spider_coeff_dir + \"17_12_mean_coeffs_combined.npy\", mean_coeffs_configs)\n",
    "# np.save(spider_coeff_dir + \"01_08_90180_SEM_coeffs_combined.npy\", SEMs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_coeffs_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fisher Info - Variance min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check error bars with fisher info - covariance matrix diag  gives us the lower bound on the variance \n",
    "# of the corresponding parameters. This should be evaluated with parameters corresponding to the \n",
    "# maximum likelihood (i.e. after optimisation).\n",
    "\n",
    "# # 1. Get optimal parameters\n",
    "# # Based on mean fits\n",
    "# mean_pos_configs =  np.mean(final_pos_configs, axis=1)\n",
    "# mean_flux_configs =   np.mean(final_flux_configs, axis=1)\n",
    "# final_models = [dl.Telescope(optics_sp180, ('source', red_src)),\n",
    "#                 dl.Telescope(optics_sp270, ('source', red_src)),\n",
    "#                 dl.Telescope(optics_sp180, ('source', red_src)),\n",
    "#                 ]\n",
    "# np.save(spider_coeff_dir + \"final_model_mean_pos.npy\", mean_pos_configs)\n",
    "# np.save(spider_coeff_dir + \"final_model_mean_flux.npy\", mean_flux_configs)\n",
    "\n",
    "\n",
    "# for i, mean_coeffs in enumerate(mean_coeffs_configs):\n",
    "#     final_models[i] = final_models[i].set('aperture.coefficients', mean_coeffs)\n",
    "#     final_models[i] = final_models[i].set('source.position', np.asarray([mean_pos_configs[i]]))\n",
    "#     final_models[i] = final_models[i].set('source.flux', np.asarray([mean_flux_configs[i]]))\n",
    "    \n",
    "# # 2. Calc covariance matrix\n",
    "# fisher_stdevs = []\n",
    "# for model in final_models:\n",
    "#     cov = zdx.self_covariance_matrix(model, \"aperture.coefficients\", zdx.bayes.poiss_loglike)\n",
    "#     print(\"Fisher info var: sqrt(Diag)/N_frames: {}\".format((np.diag(cov)**0.5)/n_frames))\n",
    "\n",
    "#     fisher_stdevs.append((np.diag(cov)**0.5)/n_frames)\n",
    "# # 1. Get optimal parameters\n",
    "# Based on mean fits\n",
    "mean_pos_configs =  np.mean(final_pos_configs[-1], axis=0)\n",
    "mean_flux_configs =   np.mean(final_flux_configs[-1], axis=0)\n",
    "final_model = dl.Telescope(optics_sp180, ('source', red_src))\n",
    "\n",
    "np.save(spider_coeff_dir + \"final_model_mean_pos.npy\", mean_pos_configs)\n",
    "np.save(spider_coeff_dir + \"final_model_mean_flux.npy\", mean_flux_configs)\n",
    "\n",
    "final_model = final_model.set('aperture.coefficients', mean_coeffs_configs)\n",
    "final_model = final_model.set('source.position', np.asarray([mean_pos_configs]))\n",
    "final_model = final_model.set('source.flux', np.asarray([mean_flux_configs]))\n",
    "    \n",
    "# 2. Calc covariance matrix\n",
    "cov = zdx.self_covariance_matrix(final_model, \"aperture.coefficients\", zdx.bayes.poiss_loglike)\n",
    "print(\"Fisher info var: sqrt(Diag)/N_frames: {}\".format((np.diag(cov)**0.5)/n_frames))\n",
    "\n",
    "fisher_stdevs = (np.diag(cov)**0.5)/n_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRLB is slightly larger than our calculated uncertainties. Lets use this theoretical limit instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt_markers = ['.','*','x']\n",
    "# plt.figure(figsize=(10,5))\n",
    "# for i, mean_coeffs_config in enumerate(mean_coeffs_configs):\n",
    "#     plt.errorbar(zernike_indicies, mean_coeffs_config, yerr=fisher_stdevs[i], fmt=fmt_markers[i], capsize=4, label=config_labels[i])\n",
    "\n",
    "# plt.title(\"Summary Coefficients: n = \" + str(n_frames)+ \" frames\")\n",
    "# plt.xlabel(\"Zernike Noll Idx\")\n",
    "# plt.ylabel(\"Mean Coefficient (m)\")\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.errorbar(zernike_indicies, mean_coeffs_configs, yerr=fisher_stdevs, fmt=fmt_markers[0], capsize=4, label=\"Combined\")\n",
    "plt.title(\"Summary Coefficients: n = \" + str(n_frames)+ \" frames\")\n",
    "plt.xlabel(\"Zernike Noll Idx\")\n",
    "plt.ylabel(\"Mean Coefficient (m)\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toliman_dp_design",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
